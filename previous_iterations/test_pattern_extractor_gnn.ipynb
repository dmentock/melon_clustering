{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_clusterssentence_clusters [[['Krass das habe ich noch nie gesehen', 'Krass'], ['Krass das war wirklich beeindruckend', 'Krass'], ['Krass wie schnell du das geschafft hast', 'Krass']], [['Das war eine krass schwierige Entscheidung', 'krass'], ['Er hatte eine krass interessante Idee', 'krass'], ['Die krasse VerÃ¤nderung hat alle Ã¼berrascht', 'krasse']], [['Die Party war krass gut organisiert', 'krass'], ['Er hat das krass schnell erledigt', 'krass'], ['Sie war krass begeistert von dem Ergebnis', 'krass'], ['Der Vortrag war krass langweilig', 'krass']]]\n",
      "sentence_clusterssentence_clusters [[('Finde ich schon <ROOT>', 'krass')], [('Das war <ROOT>', 'krass')], [('Amerikanische Universitäten sind <ROOT> elitär', 'krass')], [('normalen Jar Jar nicht den <ROOT> Sith Jar Jar Das Ding ist aber Mit all diesen Entscheidungen', 'krassen')], [('dieses <ROOT> Gefühl', 'krasse')]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 104\u001b[0m\n\u001b[0;32m     93\u001b[0m                 vectors_reference \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39misolate_reference_embeddings(embeddings, num_additional_sentences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(sentences_dict_db[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkrass\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m     94\u001b[0m                 clustering_results \u001b[38;5;241m=\u001b[39m cluster_manager\u001b[38;5;241m.\u001b[39mrun_all_configurations(\n\u001b[0;32m     95\u001b[0m                     vectors\u001b[38;5;241m=\u001b[39mvectors_reference,\n\u001b[0;32m     96\u001b[0m                     dim_methods\u001b[38;5;241m=\u001b[39mdim_reduction_methods,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    101\u001b[0m                     annotate_plt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    102\u001b[0m                 )\n\u001b[1;32m--> 104\u001b[0m                 \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_multiple_configurations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclustering_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref_word\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverlap_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moverlap_threshold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_sentences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_sentences\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigmoid_steepness\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msigmoid_steepness\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Export all results to a CSV\u001b[39;00m\n\u001b[0;32m    107\u001b[0m df \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mexport_results()\n",
      "File \u001b[1;32m~\\Music\\melon_clustering\\melon_clustering\\cluster_evaluator.py:96\u001b[0m, in \u001b[0;36mevaluate_multiple_configurations\u001b[1;34m(self, clustering_results, ref_word, plot, save, **extra_info)\u001b[0m\n\u001b[0;32m     94\u001b[0m     generated_clusters_list \u001b[38;5;241m=\u001b[39m [cluster \u001b[38;5;28;01mfor\u001b[39;00m cluster \u001b[38;5;129;01min\u001b[39;00m generated_clusters\u001b[38;5;241m.\u001b[39mvalues()]\n\u001b[0;32m     95\u001b[0m     similarity_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompare_clusters(generated_clusters_list, reference_clusters_ids)\n\u001b[1;32m---> 96\u001b[0m     avg_similarity \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean([score \u001b[38;5;28;01mfor\u001b[39;00m _, _, score \u001b[38;5;129;01min\u001b[39;00m similarity_scores])\n\u001b[0;32m     97\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDimensionality Reduction\u001b[39m\u001b[38;5;124m'\u001b[39m: dim_method,\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClustering Method\u001b[39m\u001b[38;5;124m'\u001b[39m: cluster_method,\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Jaccard Similarity\u001b[39m\u001b[38;5;124m'\u001b[39m: avg_similarity\n\u001b[0;32m    101\u001b[0m     })\n\u001b[0;32m    102\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\n",
      "File \u001b[1;32m~\\Music\\melon_clustering\\melon_clustering\\cluster_evaluator.py:53\u001b[0m, in \u001b[0;36mprepare_reference_clusters_ids\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     51\u001b[0m                 cluster_ids\u001b[38;5;241m.\u001b[39madd(sid)\n\u001b[0;32m     52\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     ref_clusters_ids\u001b[38;5;241m.\u001b[39mappend(cluster_ids)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ref_clusters_ids\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from melon_clustering import PatternExtractorGraph, ClusterEvaluator, ClusterManager, Loader, reference_clusters_de\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "\n",
    "from melon_clustering import PatternExtractorGraph, Node, CACHE_DIR\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class PatternExtractorGNN(PatternExtractorGraph):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.node_embeddings = {}\n",
    "\n",
    "    def set_up_digraph(self):\n",
    "        for node in self.id_to_node.values():\n",
    "            for child_node in node.children.values():\n",
    "                self.graph.add_edge(node.id, child_node.id)\n",
    "\n",
    "    def build_graph(self):\n",
    "        edge_index = torch.tensor(list(self.graph.edges)).t().contiguous()\n",
    "        return edge_index\n",
    "\n",
    "    def initialize_node_features(self, feature_dim=100):\n",
    "        num_nodes = self.node_counter\n",
    "        node_features = torch.randn((num_nodes, feature_dim), requires_grad=True)\n",
    "        return node_features\n",
    "\n",
    "    def initialize_node_embeddings(self, hidden_dim=64, output_dim=100, epochs=200):\n",
    "        extractor.set_up_digraph()\n",
    "        edge_index = self.build_graph()\n",
    "        node_features = self.initialize_node_features(feature_dim=100)\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = GCN(input_dim=node_features.shape[1], hidden_dim=hidden_dim, output_dim=output_dim).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "        edge_index = edge_index.to(device)\n",
    "        node_features = node_features.to(device)\n",
    "        model.train()\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            out = model(node_features, edge_index)\n",
    "            loss = F.mse_loss(out, node_features)  # Unsupervised loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        self.node_embeddings = out.detach().cpu().numpy()\n",
    "\n",
    "    def compute_weighted_sentence_embedding(self, sentence_path, steepness=1.0):\n",
    "        weighted_embedding = np.zeros(self.node_embeddings.shape[1])\n",
    "        for i, node_id in enumerate(sentence_path):\n",
    "            weight = 1 - (1 / (1 + np.exp(-steepness * (i + 1))))\n",
    "            weighted_embedding += weight * self.node_embeddings[node_id]\n",
    "        return weighted_embedding / len(sentence_path)\n",
    "\n",
    "dim_reduction_methods = ['LSA', 'PCA', 't-SNE', 'MDS']\n",
    "clustering_methods = ['KMeans', 'Agglomerative', 'DBSCAN']\n",
    "cluster_manager = ClusterManager()\n",
    "\n",
    "results_collection = []\n",
    "\n",
    "for sigmoid_steepness in [0.2, 0.4, 0.6, 0.8, 1]:\n",
    "    for overlap_threshold in [0.2, 0.4, 0.6, 0.8, 1]:\n",
    "        for n_sentences in [5, 50, 500, 5000, 10000]:\n",
    "# for sigmoid_steepness in [0.2]:\n",
    "#     for overlap_threshold in [0.2]:\n",
    "#         for n_sentences in [5]:\n",
    "            for ref_word, reference_clusters in reference_clusters_de.items():\n",
    "                additional_sentences_dict = Loader.load_sentences_from_word(ref_word, 'de', n_sentences=n_sentences)\n",
    "                evaluator = ClusterEvaluator(reference_clusters=reference_clusters)\n",
    "                sentences_dict_ref = evaluator.add_reference_sentences(reference_clusters)\n",
    "                sentences_dict_db = evaluator.add_additional_sentences(additional_sentences_dict)\n",
    "\n",
    "                extractor = PatternExtractorGNN()\n",
    "                extractor.initialize(evaluator.sentences_dict, overlap_threshold=overlap_threshold)\n",
    "                extractor.initialize_node_embeddings()\n",
    "                embeddings, sentences_list = extractor.get_all_sentence_embeddings(evaluator.sentences_dict, steepness=sigmoid_steepness)\n",
    "\n",
    "                vectors_reference = evaluator.isolate_reference_embeddings(embeddings, num_additional_sentences=len(sentences_dict_db['krass']))\n",
    "                reference_sentence_ids = sorted(evaluator.reference_sentence_ids)\n",
    "                reference_sentence_paths = [(sid, evaluator.sentence_id_to_original[sid]) for sid in reference_sentence_ids]\n",
    "\n",
    "                clustering_results = cluster_manager.run_all_configurations(\n",
    "                    vectors=vectors_reference,\n",
    "                    dim_methods=dim_reduction_methods,\n",
    "                    cluster_methods=clustering_methods,\n",
    "                    sentence_paths=reference_sentence_paths,\n",
    "                    plot_seaborn=False,\n",
    "                    plot_plt=False,\n",
    "                    annotate_plt=True\n",
    "                )\n",
    "\n",
    "                results_df = evaluator.evaluate_multiple_configurations(clustering_results, ref_word, plot=False,\n",
    "                                                                        overlap_threshold=str(overlap_threshold),\n",
    "                                                                        n_sentences=str(n_sentences),\n",
    "                                                                        sigmoid_steepness=str(sigmoid_steepness))\n",
    "\n",
    "                # Add additional information to each result and store in a list\n",
    "                for index, row in results_df.iterrows():\n",
    "                    row_data = {\n",
    "                        'Ref Word': ref_word,\n",
    "                        'Overlap Threshold': overlap_threshold,\n",
    "                        'Num Sentences': n_sentences,\n",
    "                        'Sigmoid Steepness': sigmoid_steepness,\n",
    "                        'Dimensionality Reduction': row['Dimensionality Reduction'],\n",
    "                        'Clustering Method': row['Clustering Method'],\n",
    "                        'Average Jaccard Similarity': row['Average Jaccard Similarity']\n",
    "                    }\n",
    "                    results_collection.append(row_data)\n",
    "\n",
    "df = pd.DataFrame(results_collection)\n",
    "df.to_csv(CACHE_DIR / 'clustering_results.csv', index=False)\n",
    "df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
