{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.016805648803710938\n",
      "2 0.2829623222351074\n",
      "3 0.41954851150512695\n",
      "4 0.42424726486206055\n",
      "5 0.4292476177215576\n",
      "6 3.4502573013305664\n",
      "DBManager initializing...\n",
      "2024-07-27 06:54:50,643 DEBUG Initializing DBManager\n",
      "2024-07-27 06:54:50,769 DEBUG DBManager initialized successfully\n",
      "res [('優位', '彼らにとって 優位に働く｡'), ('優位', '非常に優位性が高い'), ('優位', '項羽が優位だったってこと？と思うじゃないですか。\\r 逆じゃない？ '), ('優位', '2 心配事やストレス\\r 心配事やストレスがあると交感神経が優位 '), ('優位', '一方ストローズはアメリカの優位性を維持するために 原爆よりも強い爆弾核融合爆弾 '), ('優位', '国単位で見ればより強い核を持つほど圧倒的優位な立場に立てるので '), ('優位', 'しかし それでは選ばれしものという優位性が…'), ('優位', '副交感神経が優位になると '), ('優位', '睡眠って副交感神経が優位な状態で '), ('優位', '副交感神経優位になるので '), ('優位', 'saiが優位に 事を運んでいるみたいだったのに'), ('優位', '数的優位でも不利でも ちゃんとサッカーができるか'), ('優位', '女性たちが少々優位なだけです'), ('優位', 'その絶対的優位こそ'), ('優位', 'いまいましい分家筋より 完全に優位に立てる'), ('優位', 'それがあるだけで ボーシヤたちの立場は優位'), ('優位', 'スパイラル っちゅうチームは 車の優位性に懸ける戦略ですから'), ('優位', '圧倒的優位は 動かんで！'), ('優位', '圧倒的優位は動かんのや'), ('優位', '優位に立てると思ったら 大間違いだ')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from melon_db import Parser\n",
    "\n",
    "word = '優位'\n",
    "lemmas = [word]\n",
    "lang =  'jp'\n",
    "parser = Parser(lang)\n",
    "\n",
    "res = parser.get_query(\n",
    "    \"\"\"\n",
    "    WITH RankedSegments AS (\n",
    "        SELECT l.lemma, ts.text, w.word, sn.name AS source_name, se.name AS source_entry_name, ts.index,\n",
    "                ROW_NUMBER() OVER (PARTITION BY l.lemma, sn.name, sn.domain_subcategory_id ORDER BY se.index, ts.index) AS rn\n",
    "        FROM text_segments ts\n",
    "        JOIN source_entries se ON ts.source_entry_id = se.id\n",
    "        JOIN words_in_text_segments wits ON ts.id = wits.text_segment_id\n",
    "        JOIN words w ON wits.word_id = w.id\n",
    "        JOIN lemmas l ON w.lemma_id = l.id\n",
    "        JOIN source_names sn ON se.source_name_id = sn.id\n",
    "        WHERE l.lemma = %s AND sn.lang = %s)\n",
    "    SELECT word, text\n",
    "    FROM RankedSegments\n",
    "    ORDER BY lemma, source_entry_name, index LIMIT 20;\n",
    "\"\"\", (word, lang)\n",
    ")\n",
    "print(\"res\",res)\n",
    "# import cluster\n",
    "\n",
    "# c = cluster.Cluster(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "# c.cluster_sentences(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "server closed the connection unexpectedly\n\tThis probably means the server terminated abnormally\n\tbefore or while processing the request.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;43;03m    WITH RankedSegments AS (\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;43;03m        SELECT l.lemma, ts.text, w.word, sn.name AS source_name, se.name AS source_entry_name, ts.index,\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;43;03m                ROW_NUMBER() OVER (PARTITION BY l.lemma, sn.name, sn.domain_subcategory_id ORDER BY se.index, ts.index) AS rn\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;43;03m        FROM text_segments ts\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;43;03m        JOIN source_entries se ON ts.source_entry_id = se.id\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;43;03m        JOIN words_in_text_segments wits ON ts.id = wits.text_segment_id\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;43;03m        JOIN words w ON wits.word_id = w.id\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;43;03m        JOIN lemmas l ON w.lemma_id = l.id\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;43;03m        JOIN source_names sn ON se.source_name_id = sn.id\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;43;03m        WHERE l.lemma = %s AND sn.lang = %s)\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;43;03m    SELECT word, text\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;43;03m    FROM RankedSegments\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;43;03m    ORDER BY lemma, source_entry_name, index LIMIT 20;\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Music\\melon_db\\melon_db\\source_parser.py:201\u001b[0m, in \u001b[0;36mParser.get_query\u001b[1;34m(self, query, items)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, items\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitems\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitems\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [a \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb\u001b[38;5;241m.\u001b[39mcur\u001b[38;5;241m.\u001b[39mfetchall()]\n",
      "\u001b[1;31mOperationalError\u001b[0m: server closed the connection unexpectedly\n\tThis probably means the server terminated abnormally\n\tbefore or while processing the request.\n"
     ]
    }
   ],
   "source": [
    "parser.get_query(\n",
    "    \"\"\"\n",
    "    WITH RankedSegments AS (\n",
    "        SELECT l.lemma, ts.text, w.word, sn.name AS source_name, se.name AS source_entry_name, ts.index,\n",
    "                ROW_NUMBER() OVER (PARTITION BY l.lemma, sn.name, sn.domain_subcategory_id ORDER BY se.index, ts.index) AS rn\n",
    "        FROM text_segments ts\n",
    "        JOIN source_entries se ON ts.source_entry_id = se.id\n",
    "        JOIN words_in_text_segments wits ON ts.id = wits.text_segment_id\n",
    "        JOIN words w ON wits.word_id = w.id\n",
    "        JOIN lemmas l ON w.lemma_id = l.id\n",
    "        JOIN source_names sn ON se.source_name_id = sn.id\n",
    "        WHERE l.lemma = %s AND sn.lang = %s)\n",
    "    SELECT word, text\n",
    "    FROM RankedSegments\n",
    "    ORDER BY lemma, source_entry_name, index LIMIT 20;\n",
    "\"\"\", (word, lang))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.0010006427764892578\n",
      "2 0.03171038627624512\n",
      "3 0.06251883506774902\n",
      "4 0.06351923942565918\n",
      "5 0.06452226638793945\n",
      "6 0.6295032501220703\n",
      "DBManager initializing...\n",
      "2024-09-01 20:48:20,678 DEBUG Cluster class initialized with model: None\n",
      "2024-09-01 20:48:20,693 DEBUG Clustering sentences\n",
      "2024-09-01 20:48:20,694 DEBUG Generating embeddings for target_word: umgezogen, text: Sind so oft umgezogen\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 18\u001b[0m\n\u001b[0;32m      3\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      4\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mumgezogen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSind so oft umgezogen\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      5\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mumgezogen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVor acht Jahren sind sie noch mal umgezogen\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Add all your sentences here...\u001b[39;00m\n\u001b[0;32m     15\u001b[0m ]\n\u001b[0;32m     17\u001b[0m clustering \u001b[38;5;241m=\u001b[39m Clustering(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mde\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m \u001b[43mclustering\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcluster_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mops\\Music\\melon_db\\scripts\\cluster.py:123\u001b[0m, in \u001b[0;36mClustering.cluster_sentences\u001b[1;34m(self, sentences, max_clusters)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcluster_sentences\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentences, max_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClustering sentences\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 123\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_embeddings_for_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,embeddings)\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;66;03m# if np.any(np.isnan(embeddings)):\u001b[39;00m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;66;03m#     self.log.warning(\"NaN values detected in embeddings. Please check the input sentences and target words.\")\u001b[39;00m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;66;03m#     print(\"NaN values detected in embeddings. Please check the input sentences and target words.\")\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mops\\Music\\melon_db\\scripts\\cluster.py:84\u001b[0m, in \u001b[0;36mClustering.generate_embeddings_for_sentences\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m     82\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target_word, text \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[1;32m---> 84\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_word\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m         embeddings\u001b[38;5;241m.\u001b[39mappend(embedding)  \u001b[38;5;66;03m# Convert tensor to numpy array\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mops\\Music\\melon_db\\scripts\\cluster.py:30\u001b[0m, in \u001b[0;36mClustering.generate_embedding\u001b[1;34m(self, target_word, text)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_embedding\u001b[39m(\u001b[38;5;28mself\u001b[39m, target_word, text):\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating embeddings for target_word: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, text: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, target_word, text)\n\u001b[1;32m---> 30\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# self.log.debug(\"Tokenized inputs: %s\", inputs)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "from cluster import Clustering\n",
    "\n",
    "sentences = [\n",
    "    ('umgezogen', 'Sind so oft umgezogen'),\n",
    "    ('umgezogen', 'Vor acht Jahren sind sie noch mal umgezogen'),\n",
    "    ('umzuziehen', 'innerhalb Frankfurts umzuziehen und Büros im Omniturm anzumieten'),\n",
    "    ('umzuziehen', 'Der Druck umzuziehen weil die Wohnung zu teuer ist'),\n",
    "    ('umzuziehen', 'In meinem Alter habe ich keine Lust nochmal umzuziehen'),\n",
    "    ('umziehe', 'Den ganzen Tag bis ich mich abends wieder umziehe'),\n",
    "    ('umziehe', 'Ich bin aufgeregt dass ich umziehe'),\n",
    "    ('umziehe', 'dass ich da mit umziehe'),\n",
    "    ('sich umzuziehen', 'Ich ziehe mich meistens um wenn es dunkel ist'),\n",
    "    ('sich umzuziehen', 'Ich komme wenn ich mich umgezogen habe'),\n",
    "    # Add all your sentences here...\n",
    "]\n",
    "\n",
    "clustering = Clustering('de')\n",
    "clustering.cluster_sentences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cluster\n",
    "\n",
    "sentences = [\n",
    "    ('umgezogen', 'Sind so oft umgezogen'),\n",
    "    ('umgezogen', 'Vor acht Jahren sind sie noch mal umgezogen'),\n",
    "    ('umzuziehen', 'innerhalb Frankfurts umzuziehen und Büros im Omniturm anzumieten'),\n",
    "    ('umzuziehen', 'Der Druck umzuziehen weil die Wohnung zu teuer ist'),\n",
    "    ('umzuziehen', 'In meinem Alter habe ich keine Lust nochmal umzuziehen'),\n",
    "    ('umziehe', 'Den ganzen Tag bis ich mich abends wieder umziehe'),\n",
    "    ('umziehe', 'Ich bin aufgeregt dass ich umziehe'),\n",
    "    ('umziehe', 'dass ich da mit umziehe'),\n",
    "    ('sich umzuziehen', 'Ich ziehe mich meistens um wenn es dunkel ist'),\n",
    "    ('sich umzuziehen', 'Ich komme wenn ich mich umgezogen habe'),\n",
    "    # Add all your sentences here...\n",
    "]\n",
    "\n",
    "cluster.cluster_sentences(sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
