{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lel <END> [{'child_structure': frozenset(), 'nodes': [<__main__.Node object at 0x00000173FD331150>, <__main__.Node object at 0x00000173FE1EF6D0>]}]\n",
      "lel vergangenheit [{'child_structure': frozenset({'<END>'}), 'nodes': [<__main__.Node object at 0x00000173FD332250>]}]\n",
      "lel <article> [{'child_structure': frozenset({'gespräch'}), 'nodes': [<__main__.Node object at 0x00000173FE210910>]}, {'child_structure': frozenset({'vergangenheit'}), 'nodes': [<__main__.Node object at 0x00000173FD3315D0>]}]\n",
      "lel an [{'child_structure': frozenset({'<article>'}), 'nodes': [<__main__.Node object at 0x00000173FD331490>, <__main__.Node object at 0x00000173ACA1AC90>]}]\n",
      "lel <ROOT> []\n",
      "lel gerne [{'child_structure': frozenset({'an'}), 'nodes': [<__main__.Node object at 0x00000173FE412850>]}]\n",
      "lel <ROOT> []\n",
      "lel an [{'child_structure': frozenset({'<article>'}), 'nodes': [<__main__.Node object at 0x00000173ACA1AC90>]}]\n",
      "lel <ROOT> []\n",
      "lel gerne [{'child_structure': frozenset({'an'}), 'nodes': [<__main__.Node object at 0x00000173FE412850>]}]\n",
      "lel <ROOT> []\n",
      "lel gespräch [{'child_structure': frozenset({'<END>'}), 'nodes': [<__main__.Node object at 0x00000173FE2A4890>]}]\n",
      "lel <article> [{'child_structure': frozenset({'gespräch'}), 'nodes': [<__main__.Node object at 0x00000173FE210910>]}, {'child_structure': frozenset({'vergangenheit'}), 'nodes': [<__main__.Node object at 0x00000173FD3315D0>]}]\n",
      "lel an [{'child_structure': frozenset({'<article>'}), 'nodes': [<__main__.Node object at 0x00000173ACA1AC90>]}]\n",
      "lel <ROOT> []\n",
      "lel gerne [{'child_structure': frozenset({'an'}), 'nodes': [<__main__.Node object at 0x00000173FE412850>]}]\n",
      "lel <ROOT> []\n",
      "[4]\n",
      "lel <END> [{'child_structure': frozenset(), 'nodes': [<__main__.Node object at 0x00000173FD342010>, <__main__.Node object at 0x00000173FD330710>]}]\n",
      "lel vergangenheit [{'child_structure': frozenset({'<END>'}), 'nodes': [<__main__.Node object at 0x00000173FD330D50>]}]\n",
      "lel <article> [{'child_structure': frozenset({'gespräch'}), 'nodes': [<__main__.Node object at 0x00000173FE2A4AD0>]}, {'child_structure': frozenset({'vergangenheit'}), 'nodes': [<__main__.Node object at 0x00000173FD3326D0>]}]\n",
      "lel an [{'child_structure': frozenset({'<article>'}), 'nodes': [<__main__.Node object at 0x00000173FE1EE210>, <__main__.Node object at 0x00000173FD332F50>]}]\n",
      "lel <ROOT> []\n",
      "lel gerne [{'child_structure': frozenset({'an'}), 'nodes': [<__main__.Node object at 0x00000173FD332250>]}]\n",
      "lel <ROOT> []\n",
      "lel an [{'child_structure': frozenset({'<article>'}), 'nodes': [<__main__.Node object at 0x00000173FE1EE210>]}]\n",
      "lel <ROOT> []\n",
      "lel gerne [{'child_structure': frozenset({'an'}), 'nodes': [<__main__.Node object at 0x00000173FD332250>]}]\n",
      "lel <ROOT> []\n",
      "lel gespräch [{'child_structure': frozenset({'<END>'}), 'nodes': [<__main__.Node object at 0x00000173FD27A610>]}]\n",
      "lel <article> [{'child_structure': frozenset({'gespräch'}), 'nodes': [<__main__.Node object at 0x00000173FE2A4AD0>]}, {'child_structure': frozenset({'vergangenheit'}), 'nodes': [<__main__.Node object at 0x00000173FD3326D0>]}]\n",
      "lel an [{'child_structure': frozenset({'<article>'}), 'nodes': [<__main__.Node object at 0x00000173FE1EE210>]}]\n",
      "lel <ROOT> []\n",
      "lel gerne [{'child_structure': frozenset({'an'}), 'nodes': [<__main__.Node object at 0x00000173FD332250>]}]\n",
      "lel <ROOT> []\n",
      "[4]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, word, id):\n",
    "        self.word = word\n",
    "        self.id = id\n",
    "        self.count = 0\n",
    "        self.children = {}\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, word, id):\n",
    "        self.word = word\n",
    "        self.id = id\n",
    "        self.count = 0\n",
    "        self.children = {}\n",
    "\n",
    "\n",
    "class PatternExtractor:\n",
    "    def __init__(self):\n",
    "        self.preceding_tree = Node('<ROOT>', 0)\n",
    "        self.following_tree = Node('<ROOT>', 1)\n",
    "        self.node_counter = 2\n",
    "        self.word_to_preceding_ids = defaultdict(list)\n",
    "        self.word_to_following_ids = defaultdict(list)\n",
    "        self.id_to_node = {0: self.preceding_tree, 1: self.following_tree}\n",
    "        self.child_to_parents = defaultdict(list)\n",
    "        self.node_embeddings = {}  # Store embeddings for each node\n",
    "\n",
    "    def add_start_end_flags_lower(self, sentences):\n",
    "        return [f\"<START> {sentence.lower()} <END>\" for sentence in sentences]\n",
    "\n",
    "    def get_or_create_node(self, current_tree, word, parent_id, direction):\n",
    "        word_to_ids = self.word_to_preceding_ids if direction == 'preceding' else self.word_to_following_ids\n",
    "        if word not in current_tree.children:\n",
    "            new_node = Node(word, self.node_counter)\n",
    "            current_tree.children[word] = new_node\n",
    "            word_to_ids[word].append(self.node_counter)\n",
    "            self.id_to_node[self.node_counter] = new_node\n",
    "            if parent_id is not None:\n",
    "                self.child_to_parents[self.node_counter].append(parent_id)\n",
    "            self.node_counter += 1\n",
    "        else:\n",
    "            child_id = current_tree.children[word].id\n",
    "            if parent_id is not None:\n",
    "                self.child_to_parents[child_id].append(parent_id)\n",
    "        return current_tree.children[word]\n",
    "\n",
    "    def add_to_tree(self, words, direction, count=1):\n",
    "        if direction == 'preceding':\n",
    "            current_tree = self.preceding_tree\n",
    "            word_range = range(len(words) - 1, -1, -1)\n",
    "        else:\n",
    "            current_tree = self.following_tree\n",
    "            word_range = range(len(words))\n",
    "\n",
    "        parent_id = current_tree.id\n",
    "\n",
    "        for i in word_range:\n",
    "            current_word = words[i]\n",
    "            current_tree = self.get_or_create_node(current_tree, current_word, parent_id, direction)\n",
    "            current_tree.count += count\n",
    "            parent_id = current_tree.id\n",
    "\n",
    "    def create_tree_mask_as_root(self, sentences_dict):\n",
    "        for morphology, sentences in sentences_dict.items():\n",
    "            sentences_with_flags = self.add_start_end_flags_lower(sentences)\n",
    "\n",
    "            for sentence in sentences_with_flags:\n",
    "                words = sentence.split()\n",
    "                key_word_index = words.index(morphology.lower())\n",
    "                words_before = words[:key_word_index]\n",
    "                words_after = words[key_word_index + 1:]\n",
    "\n",
    "                self.add_to_tree(words_before, 'preceding')\n",
    "                self.add_to_tree(words_after, 'following')\n",
    "\n",
    "    def print_tree(self, node, level=0):\n",
    "        print('  ' * level + f\"{node.word} (count: {node.count}, id: {node.id})\")\n",
    "        for child in node.children.values():\n",
    "            self.print_tree(child, level + 1)\n",
    "\n",
    "    def print_trees(self):\n",
    "        print(\"Preceding Tree (before <MASK>):\")\n",
    "        self.print_tree(self.preceding_tree)\n",
    "        print(\"\\nFollowing Tree (after <MASK>):\")\n",
    "        self.print_tree(self.following_tree)\n",
    "\n",
    "    def get_nodes_by_word(self, word, direction):\n",
    "        # Use the correct dictionary based on the direction\n",
    "        word_to_ids = self.word_to_preceding_ids if direction == 'preceding' else self.word_to_following_ids\n",
    "        ids = word_to_ids.get(word, [])\n",
    "        return {self.id_to_node[id] for id in ids}\n",
    "\n",
    "    def get_parents_by_id(self, id):\n",
    "        parent_ids = self.child_to_parents.get(id, [])\n",
    "        return [self.id_to_node[parent_id] for parent_id in parent_ids]\n",
    "\n",
    "    def optimize_tree(self, word, direction, overlap_threshold=1):\n",
    "        word_to_ids = self.word_to_preceding_ids if direction == 'preceding' else self.word_to_following_ids\n",
    "        all_nodes = self.get_nodes_by_word(word, direction)\n",
    "        groups_with_overlap_children = []\n",
    "        parent_node_ids = []\n",
    "\n",
    "        for node in all_nodes:\n",
    "            parent_node_ids.extend(self.child_to_parents[node.id])\n",
    "            child_structure = frozenset(child.word for child in node.children.values())  # Use frozenset for comparison\n",
    "\n",
    "            # Compare child structures directly\n",
    "            added_to_group = False\n",
    "            for group in groups_with_overlap_children:\n",
    "                group_structure = group['child_structure']\n",
    "                if child_structure == group_structure:  # Direct comparison of frozensets\n",
    "                    group['nodes'].append(node)\n",
    "                    added_to_group = True\n",
    "                    break\n",
    "\n",
    "            if not added_to_group:\n",
    "                groups_with_overlap_children.append({\n",
    "                    'child_structure': child_structure,\n",
    "                    'nodes': [node]\n",
    "                })\n",
    "        print(\"lel\", word, groups_with_overlap_children)\n",
    "        # Merge nodes within each group\n",
    "        new_nodes = []\n",
    "        for node_group in [group['nodes'] for group in groups_with_overlap_children if len(group['nodes']) > 1]:\n",
    "            node_with_smallest_id = min(node_group, key=lambda node: node.id)\n",
    "            new_nodes.append(node_with_smallest_id)\n",
    "\n",
    "            for node in node_group:\n",
    "                if node != node_with_smallest_id:  # node is duplicate\n",
    "                    node_with_smallest_id.children.update(node.children)  # Adopt children\n",
    "\n",
    "                    # Remove from id_to_node\n",
    "                    self.id_to_node.pop(node.id)\n",
    "                    # Remove from word_to_ids\n",
    "                    word_to_ids[word].remove(node.id)\n",
    "                    # Remove from child_to_parents, merge parents list with new main node\n",
    "                    parent_ids = self.child_to_parents.pop(node.id)\n",
    "                    for parent_id in parent_ids:\n",
    "                        if parent_id not in self.child_to_parents[node_with_smallest_id.id]:\n",
    "                            self.child_to_parents[node_with_smallest_id.id].append(parent_id)\n",
    "                        # Update parent node's reference to the new node\n",
    "                        if word in self.id_to_node.get(parent_id).children:\n",
    "                            self.id_to_node.get(parent_id).children[word] = node_with_smallest_id\n",
    "\n",
    "                    # Update children's parent references\n",
    "                    for child_node in node.children.values():\n",
    "                        self.child_to_parents[child_node.id].remove(node.id)\n",
    "                        if node_with_smallest_id.id not in self.child_to_parents[child_node.id]:\n",
    "                            self.child_to_parents[child_node.id].append(node_with_smallest_id.id)\n",
    "\n",
    "                    self.node_counter -= 1\n",
    "\n",
    "        # Recursively optimize parent nodes\n",
    "        for node_id in set(parent_node_ids):\n",
    "            if node_id in self.id_to_node:\n",
    "                self.optimize_tree(self.id_to_node[node_id].word, direction, overlap_threshold=overlap_threshold)\n",
    "\n",
    "\n",
    "    def initialize_node_embeddings(self, embedding_size=100):\n",
    "        \"\"\"\n",
    "        Initialize random embeddings for each node.\n",
    "        \"\"\"\n",
    "        for node_id in self.id_to_node.keys():\n",
    "            self.node_embeddings[node_id] = np.random.rand(embedding_size)\n",
    "\n",
    "    def compute_weighted_sentence_embedding(self, sentence_path, weighting_scheme = None):\n",
    "        \"\"\"\n",
    "        Compute a weighted sentence embedding by traversing the sentence path.\n",
    "        Nodes closer to the root are weighted more heavily.\n",
    "        \"\"\"\n",
    "        weighted_embedding = np.zeros(len(self.node_embeddings[0]))  # Assuming all embeddings have the same size\n",
    "        for i, node_id in enumerate(sentence_path):\n",
    "            if weighting_scheme == 'linear':\n",
    "                distance_from_root = i + 1\n",
    "                weight = 1 / distance_from_root\n",
    "            elif weighting_scheme == 'inverse_sigmoid':\n",
    "                distance_from_root = i + 1  # Assuming sentence_path is ordered from ROOT\n",
    "                steepness = 1\n",
    "                weight = 1 - (1 / (1 + np.exp(-steepness * distance_from_root)))\n",
    "            else:\n",
    "                weight = 1\n",
    "            weighted_embedding += weight * self.node_embeddings[node_id]\n",
    "        return weighted_embedding / len(sentence_path)\n",
    "\n",
    "    def get_sentence_embedding(self, sentence, morphology, weighting_scheme = None):\n",
    "        \"\"\"\n",
    "        Given a sentence, return its corresponding embedding.\n",
    "        \"\"\"\n",
    "\n",
    "        words = sentence.lower().split()\n",
    "        node_path = deque([0])\n",
    "        key_word_index = words.index(morphology.lower())\n",
    "        words_before = words[:key_word_index]\n",
    "        words_after = words[key_word_index + 1:]\n",
    "\n",
    "        current_node = self.preceding_tree\n",
    "        for i in range(len(words_before) - 1, -1, -1):\n",
    "            word = words_before[i]\n",
    "            if word in current_node.children:\n",
    "                current_node = current_node.children[word]\n",
    "                node_path.appendleft(current_node.id)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        current_node = self.following_tree\n",
    "        for word in words_after:\n",
    "            if word in current_node.children:\n",
    "                current_node = current_node.children[word]\n",
    "                node_path.append(current_node.id)\n",
    "            else:\n",
    "                raise\n",
    "        return self.compute_weighted_sentence_embedding(node_path, weighting_scheme=weighting_scheme)\n",
    "\n",
    "    def get_all_sentence_embeddings(self, sentences_dict, weighting_scheme=None):\n",
    "        \"\"\"\n",
    "        Generate embeddings for all sentences in the dataset.\n",
    "        \"\"\"\n",
    "        sentence_embeddings = []\n",
    "        sentence_list = []\n",
    "        for morphology, sentences in sentences_dict.items():\n",
    "            sentences_with_flags = self.add_start_end_flags_lower(sentences)\n",
    "            for sentence in sentences_with_flags:\n",
    "                embedding = self.get_sentence_embedding(sentence, morphology, weighting_scheme=weighting_scheme)\n",
    "                sentence_embeddings.append(embedding)\n",
    "                sentence_list.append(sentence)\n",
    "        return np.array(sentence_embeddings), sentence_list\n",
    "\n",
    "    def cluster_embeddings(self, embeddings, n_clusters=3):\n",
    "        # Apply K-Means clustering to the embeddings\n",
    "        kmeans = KMeans(n_clusters=n_clusters)\n",
    "        clusters = kmeans.fit_predict(embeddings)\n",
    "        return clusters\n",
    "\n",
    "    def reduce_dimensionality(self, embeddings, method='pca', n_components=2):\n",
    "        # Reduce dimensionality using PCA or t-SNE for visualization\n",
    "        if method == 'pca':\n",
    "            reducer = PCA(n_components=n_components)\n",
    "        elif method == 'tsne':\n",
    "            perplexity = min(30, len(embeddings) - 1)\n",
    "            reducer = TSNE(n_components=n_components, perplexity=perplexity)\n",
    "\n",
    "        reduced_embeddings = reducer.fit_transform(embeddings)\n",
    "        return reduced_embeddings\n",
    "    def visualize_clusters(self, reduced_embeddings, clusters, sentence_list=None, method='pca', appendix=None):\n",
    "        # Visualize clusters using a 2D scatter plot\n",
    "        plt.figure(figsize=(9, 9))\n",
    "        scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=clusters, cmap='viridis')\n",
    "\n",
    "        # Annotate sentences for better interpretability\n",
    "        if sentence_list:\n",
    "            for i, sentence in enumerate(sentence_list):\n",
    "                plt.annotate(sentence, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]), fontsize=8, alpha=0.7)\n",
    "\n",
    "        plt.title(f\"Sentence Clustering Visualization ({method}{' ' + str(appendix) if appendix else ''})\")\n",
    "        plt.xlabel(f\"{method.upper()} Component 1\")\n",
    "        plt.ylabel(f\"{method.upper()} Component 2\")\n",
    "        plt.show()\n",
    "\n",
    "    def initialize(self, sentences_dict, overlap_threshold = 1):\n",
    "        self.create_tree_mask_as_root(sentences_dict)\n",
    "        # self.optimize_tree('<START>', 'preceding', overlap_threshold=overlap_threshold)\n",
    "        self.optimize_tree('<END>', 'forward', overlap_threshold=overlap_threshold)\n",
    "\n",
    "# Create a PatternExtractor instance\n",
    "sentences_dict = {\n",
    "    'erinnert': [\n",
    "        'sie erinnert an <ARTICLE> Gespräch',\n",
    "        'er erinnert gerne an <ARTICLE> Vergangenheit'\n",
    "    ],\n",
    "}\n",
    "extractor = PatternExtractor()\n",
    "extractor.initialize(sentences_dict, overlap_threshold = 0.4)\n",
    "print(extractor.word_to_following_ids['an']) # should print one id\n",
    "\n",
    "extractor = PatternExtractor()\n",
    "extractor.initialize(sentences_dict, overlap_threshold = 0.6)\n",
    "print(extractor.word_to_following_ids['an']) # should print two ids\n",
    "\n",
    "# extractor.initialize(sentences_dict, overlap_threshold=1)\n",
    "# extractor.print_trees()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sentences_dict = {\n",
    "#     'erinnere': [\n",
    "#         'Ich erinnere mich gut',\n",
    "#         'ich erinnere mich nicht',\n",
    "#         'nochmal erinnere ich mich nicht',\n",
    "#         'ich erinnere mich an das Treffen gestern',\n",
    "#         'erinnere mich bitte daran, morgen früh aufzustehen',\n",
    "#         'ich erinnere mich an die schöne Zeit'\n",
    "#     ],\n",
    "#     'erinnert': [\n",
    "#         'wie erinnert man sich nochmal',\n",
    "#         'wo erinnert man sich nochmal',\n",
    "#         'vielleicht erinnert man sich dann nochmal',\n",
    "#         'erinnert mich an meine Kindheit',\n",
    "#         'sie erinnert sich nicht mehr an das Gespräch',\n",
    "#         'er erinnert sich nicht gerne an die Vergangenheit'\n",
    "#     ],\n",
    "#     'erinnerte': [\n",
    "#         'er erinnerte sich plötzlich an den Vorfall',\n",
    "#         'ich erinnerte mich an mein erstes Auto',\n",
    "#         'sie erinnerte sich, dass sie etwas vergessen hatte',\n",
    "#         'erinnerte ich mich an den alten Freund',\n",
    "#         'er erinnerte sich an die Worte seiner Mutter',\n",
    "#         'ich erinnerte mich an den letzten Urlaub'\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# Build the trees from the sentences\n",
    "# extractor.create_tree_mask_as_root(sentences_dict)\n",
    "\n",
    "\n",
    "extractor.initialize(sentences_dict, overlap_threshold=0.4)\n",
    "\n",
    "extractor.print_trees()\n",
    "\n",
    "# # Initialize embeddings\n",
    "# extractor.initialize_node_embeddings(embedding_size=500)\n",
    "# extractor.id_to_node\n",
    "# # Step 1: Generate embeddings for all sentences\n",
    "\n",
    "# for weighting_scheme in ['no distance weighting', 'linear', 'inverse_sigmoid']:\n",
    "#     sentence_embeddings, sentence_list = extractor.get_all_sentence_embeddings(sentences_dict, weighting_scheme=weighting_scheme)\n",
    "#     n_clusters = 3  # You can adjust this based on the number of clusters you want\n",
    "#     clusters = extractor.cluster_sentences(sentence_embeddings, n_clusters=n_clusters)\n",
    "#     reduced_embeddings = extractor.reduce_dimensionality(sentence_embeddings)\n",
    "#     extractor.visualize_clusters(reduced_embeddings, clusters, sentence_list, title_appendix=weighting_scheme)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
