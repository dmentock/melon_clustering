{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preceding Tree (before <MASK>):\n",
      "<ROOT> (count: 0, id: 0)\n",
      "  ich (count: 2, id: 2)\n",
      "    <START> (count: 2, id: 3)\n",
      "  nochmal (count: 1, id: 9)\n",
      "    <START> (count: 1, id: 10)\n",
      "  wie (count: 1, id: 15)\n",
      "    <START> (count: 1, id: 16)\n",
      "  wo (count: 1, id: 21)\n",
      "    <START> (count: 1, id: 22)\n",
      "  vielleicht (count: 1, id: 23)\n",
      "    <START> (count: 1, id: 24)\n",
      "\n",
      "Following Tree (after <MASK>):\n",
      "<ROOT> (count: 0, id: 1)\n",
      "  mich (count: 2, id: 4)\n",
      "    gut (count: 1, id: 5)\n",
      "      <END> (count: 1, id: 6)\n",
      "    nicht (count: 1, id: 7)\n",
      "      <END> (count: 1, id: 8)\n",
      "  ich (count: 1, id: 11)\n",
      "    mich (count: 1, id: 12)\n",
      "      nicht (count: 1, id: 7)\n",
      "        <END> (count: 1, id: 8)\n",
      "  man (count: 3, id: 17)\n",
      "    sich (count: 3, id: 18)\n",
      "      nochmal (count: 2, id: 19)\n",
      "        <END> (count: 2, id: 20)\n",
      "      dann (count: 1, id: 25)\n",
      "        nochmal (count: 2, id: 19)\n",
      "          <END> (count: 2, id: 20)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, word, id):\n",
    "        self.word = word\n",
    "        self.id = id\n",
    "        self.count = 0\n",
    "        self.children = {}\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, word, id):\n",
    "        self.word = word\n",
    "        self.id = id\n",
    "        self.count = 0\n",
    "        self.children = {}\n",
    "\n",
    "\n",
    "class PatternExtractor:\n",
    "    def __init__(self):\n",
    "        self.preceding_tree = Node('<ROOT>', 0)\n",
    "        self.following_tree = Node('<ROOT>', 1)\n",
    "        self.node_counter = 2\n",
    "        self.word_to_preceding_ids = defaultdict(list)\n",
    "        self.word_to_following_ids = defaultdict(list)\n",
    "        self.id_to_node = {0: self.preceding_tree, 1: self.following_tree}\n",
    "        self.child_to_parents = defaultdict(list)\n",
    "        self.node_embeddings = {}  # Store embeddings for each node\n",
    "\n",
    "    def add_start_end_flags_lower(self, sentences):\n",
    "        return [f\"<START> {sentence.lower()} <END>\" for sentence in sentences]\n",
    "\n",
    "    def get_or_create_node(self, current_tree, word, parent_id, direction):\n",
    "        word_to_ids = self.word_to_preceding_ids if direction == 'preceding' else self.word_to_following_ids\n",
    "        if word not in current_tree.children:\n",
    "            new_node = Node(word, self.node_counter)\n",
    "            current_tree.children[word] = new_node\n",
    "            word_to_ids[word].append(self.node_counter)\n",
    "            self.id_to_node[self.node_counter] = new_node\n",
    "            if parent_id is not None:\n",
    "                self.child_to_parents[self.node_counter].append(parent_id)\n",
    "            self.node_counter += 1\n",
    "        else:\n",
    "            child_id = current_tree.children[word].id\n",
    "            if parent_id is not None:\n",
    "                self.child_to_parents[child_id].append(parent_id)\n",
    "        return current_tree.children[word]\n",
    "\n",
    "    def add_to_tree(self, words, direction, count=1):\n",
    "        if direction == 'preceding':\n",
    "            current_tree = self.preceding_tree\n",
    "            word_range = range(len(words) - 1, -1, -1)\n",
    "        else:\n",
    "            current_tree = self.following_tree\n",
    "            word_range = range(len(words))\n",
    "\n",
    "        parent_id = current_tree.id\n",
    "\n",
    "        for i in word_range:\n",
    "            current_word = words[i]\n",
    "            current_tree = self.get_or_create_node(current_tree, current_word, parent_id, direction)\n",
    "            current_tree.count += count\n",
    "            parent_id = current_tree.id\n",
    "\n",
    "    def create_tree_mask_as_root(self, sentences_dict):\n",
    "        for morphology, sentences in sentences_dict.items():\n",
    "            sentences_with_flags = self.add_start_end_flags_lower(sentences)\n",
    "\n",
    "            for sentence in sentences_with_flags:\n",
    "                words = sentence.split()\n",
    "                key_word_index = words.index(morphology.lower())\n",
    "                words_before = words[:key_word_index]\n",
    "                words_after = words[key_word_index + 1:]\n",
    "\n",
    "                self.add_to_tree(words_before, 'preceding')\n",
    "                self.add_to_tree(words_after, 'following')\n",
    "\n",
    "    def print_tree(self, node, level=0):\n",
    "        print('  ' * level + f\"{node.word} (count: {node.count}, id: {node.id})\")\n",
    "        for child in node.children.values():\n",
    "            self.print_tree(child, level + 1)\n",
    "\n",
    "    def print_trees(self):\n",
    "        print(\"Preceding Tree (before <MASK>):\")\n",
    "        self.print_tree(self.preceding_tree)\n",
    "        print(\"\\nFollowing Tree (after <MASK>):\")\n",
    "        self.print_tree(self.following_tree)\n",
    "\n",
    "    def get_nodes_by_word(self, word, direction):\n",
    "        # Use the correct dictionary based on the direction\n",
    "        word_to_ids = self.word_to_preceding_ids if direction == 'preceding' else self.word_to_following_ids\n",
    "        ids = word_to_ids.get(word, [])\n",
    "        return {self.id_to_node[id] for id in ids}\n",
    "\n",
    "    def get_parents_by_id(self, id):\n",
    "        parent_ids = self.child_to_parents.get(id, [])\n",
    "        return [self.id_to_node[parent_id] for parent_id in parent_ids]\n",
    "\n",
    "    def optimize_tree(self, word, direction, overlap_threshold=1):\n",
    "        word_to_ids = self.word_to_preceding_ids if direction == 'preceding' else self.word_to_following_ids\n",
    "        all_nodes = self.get_nodes_by_word(word, direction)\n",
    "        groups_with_overlap_children = []\n",
    "        parent_node_ids = []\n",
    "        for node in all_nodes:\n",
    "            parent_node_ids.extend(self.child_to_parents[node.id])\n",
    "            child_structure = set(child.word for child in node.children.values())\n",
    "            parent_structure = set(parent.word for parent in self.get_parents_by_id(node.id))\n",
    "            combined_structure = child_structure.union(parent_structure)\n",
    "            added_to_group = False\n",
    "            for group in groups_with_overlap_children:\n",
    "                group_structure = group['combined_structure']\n",
    "                intersection = combined_structure.intersection(group_structure)\n",
    "                overlap_ratio = 0\n",
    "                if len(combined_structure) > 0 and len(group_structure) > 0:\n",
    "                    overlap_ratio = len(intersection) / max(len(combined_structure), len(group_structure))\n",
    "                if overlap_ratio >= overlap_threshold:\n",
    "                    group['nodes'].append(node)\n",
    "                    group['combined_structure'] = group['combined_structure'].union(combined_structure)\n",
    "                    added_to_group = True\n",
    "                    break\n",
    "            if not added_to_group:\n",
    "                groups_with_overlap_children.append({\n",
    "                    'combined_structure': combined_structure,\n",
    "                    'nodes': [node]\n",
    "                })\n",
    "        new_nodes = []\n",
    "        for node_group in [group['nodes'] for group in groups_with_overlap_children if len(group['nodes']) > 1]:\n",
    "            node_with_smallest_id = min(node_group, key=lambda node: node.id)\n",
    "            new_nodes.append(node_with_smallest_id)\n",
    "            for node in node_group:\n",
    "                if node != node_with_smallest_id: # node is duplicate\n",
    "                    node_with_smallest_id.children.update(node.children) #adopt children\n",
    "\n",
    "                    # remove from id_to_node\n",
    "                    self.id_to_node.pop(node.id)\n",
    "                    # remove from word_to_ids\n",
    "                    word_to_ids[word].remove(node.id)\n",
    "                    # remove from child_to_parents, merge parents list with new main node\n",
    "                    parent_ids = self.child_to_parents.pop(node.id)\n",
    "                    for parent_id in parent_ids:\n",
    "                        if parent_id not in self.child_to_parents[node_with_smallest_id.id]:\n",
    "                            self.child_to_parents[node_with_smallest_id.id].append(parent_id)\n",
    "                        # go to parent node, replace child node id\n",
    "                        self.id_to_node.get(parent_id).children[word] = node_with_smallest_id\n",
    "                    # go over children, adjust their child_to_parents list to point to new node\n",
    "                    for child_node in node.children.values():\n",
    "                        self.child_to_parents[child_node.id].remove(node.id)\n",
    "                        if node_with_smallest_id.id not in self.child_to_parents[child_node.id]:\n",
    "                            self.child_to_parents[child_node.id].append(node_with_smallest_id.id)\n",
    "                    # decrease node_counter\n",
    "                    self.node_counter -= 1\n",
    "\n",
    "        for node_id in set(parent_node_ids):\n",
    "            # Ensure the node still exists before proceeding\n",
    "            if node_id in self.id_to_node:\n",
    "                self.optimize_tree(self.id_to_node[node_id].word, direction, overlap_threshold=overlap_threshold)\n",
    "\n",
    "    def initialize_node_embeddings(self, embedding_size=100):\n",
    "        \"\"\"\n",
    "        Initialize random embeddings for each node.\n",
    "        \"\"\"\n",
    "        for node_id in self.id_to_node.keys():\n",
    "            self.node_embeddings[node_id] = np.random.rand(embedding_size)\n",
    "\n",
    "    def compute_weighted_sentence_embedding(self, sentence_path, weighting_scheme = None):\n",
    "        \"\"\"\n",
    "        Compute a weighted sentence embedding by traversing the sentence path.\n",
    "        Nodes closer to the root are weighted more heavily.\n",
    "        \"\"\"\n",
    "        weighted_embedding = np.zeros(len(self.node_embeddings[0]))  # Assuming all embeddings have the same size\n",
    "        for i, node_id in enumerate(sentence_path):\n",
    "            if weighting_scheme == 'linear':\n",
    "                distance_from_root = i + 1\n",
    "                weight = 1 / distance_from_root\n",
    "            elif weighting_scheme == 'inverse_sigmoid':\n",
    "                distance_from_root = i + 1  # Assuming sentence_path is ordered from ROOT\n",
    "                steepness = 1\n",
    "                weight = 1 - (1 / (1 + np.exp(-steepness * distance_from_root)))\n",
    "            else:\n",
    "                weight = 1\n",
    "            weighted_embedding += weight * self.node_embeddings[node_id]\n",
    "        return weighted_embedding / len(sentence_path)\n",
    "\n",
    "    def get_sentence_embedding(self, sentence, morphology, weighting_scheme = None):\n",
    "        \"\"\"\n",
    "        Given a sentence, return its corresponding embedding.\n",
    "        \"\"\"\n",
    "\n",
    "        words = sentence.lower().split()\n",
    "        node_path = deque([0])\n",
    "        key_word_index = words.index(morphology.lower())\n",
    "        words_before = words[:key_word_index]\n",
    "        words_after = words[key_word_index + 1:]\n",
    "\n",
    "        current_node = self.preceding_tree\n",
    "        for i in range(len(words_before) - 1, -1, -1):\n",
    "            word = words_before[i]\n",
    "            if word in current_node.children:\n",
    "                current_node = current_node.children[word]\n",
    "                node_path.appendleft(current_node.id)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        current_node = self.following_tree\n",
    "        for word in words_after:\n",
    "            if word in current_node.children:\n",
    "                current_node = current_node.children[word]\n",
    "                node_path.append(current_node.id)\n",
    "            else:\n",
    "                raise\n",
    "        return self.compute_weighted_sentence_embedding(node_path, weighting_scheme=weighting_scheme)\n",
    "\n",
    "    def get_all_sentence_embeddings(self, sentences_dict, weighting_scheme=None):\n",
    "        \"\"\"\n",
    "        Generate embeddings for all sentences in the dataset.\n",
    "        \"\"\"\n",
    "        sentence_embeddings = []\n",
    "        sentence_list = []\n",
    "        for morphology, sentences in sentences_dict.items():\n",
    "            sentences_with_flags = self.add_start_end_flags_lower(sentences)\n",
    "            for sentence in sentences_with_flags:\n",
    "                embedding = self.get_sentence_embedding(sentence, morphology, weighting_scheme=weighting_scheme)\n",
    "                sentence_embeddings.append(embedding)\n",
    "                sentence_list.append(sentence)\n",
    "        return np.array(sentence_embeddings), sentence_list\n",
    "\n",
    "    def cluster_embeddings(self, embeddings, n_clusters=3):\n",
    "        # Apply K-Means clustering to the embeddings\n",
    "        kmeans = KMeans(n_clusters=n_clusters)\n",
    "        clusters = kmeans.fit_predict(embeddings)\n",
    "        return clusters\n",
    "\n",
    "    def reduce_dimensionality(self, embeddings, method='pca', n_components=2):\n",
    "        # Reduce dimensionality using PCA or t-SNE for visualization\n",
    "        if method == 'pca':\n",
    "            reducer = PCA(n_components=n_components)\n",
    "        elif method == 'tsne':\n",
    "            perplexity = min(30, len(embeddings) - 1)\n",
    "            reducer = TSNE(n_components=n_components, perplexity=perplexity)\n",
    "\n",
    "        reduced_embeddings = reducer.fit_transform(embeddings)\n",
    "        return reduced_embeddings\n",
    "    def visualize_clusters(self, reduced_embeddings, clusters, sentence_list=None, method='pca', appendix=None):\n",
    "        # Visualize clusters using a 2D scatter plot\n",
    "        plt.figure(figsize=(9, 9))\n",
    "        scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=clusters, cmap='viridis')\n",
    "\n",
    "        # Annotate sentences for better interpretability\n",
    "        if sentence_list:\n",
    "            for i, sentence in enumerate(sentence_list):\n",
    "                plt.annotate(sentence, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]), fontsize=8, alpha=0.7)\n",
    "\n",
    "        plt.title(f\"Sentence Clustering Visualization ({method}{' ' + str(appendix) if appendix else ''})\")\n",
    "        plt.xlabel(f\"{method.upper()} Component 1\")\n",
    "        plt.ylabel(f\"{method.upper()} Component 2\")\n",
    "        plt.show()\n",
    "\n",
    "    def initialize(self, sentences_dict, overlap_threshold = 1):\n",
    "        self.create_tree_mask_as_root(sentences_dict)\n",
    "        self.optimize_tree('<START>', 'preceding', overlap_threshold=overlap_threshold)\n",
    "        self.optimize_tree('<END>', 'forward', overlap_threshold=overlap_threshold)\n",
    "\n",
    "# Create a PatternExtractor instance\n",
    "extractor = PatternExtractor()\n",
    "sentences_dict = {\n",
    "    'erinnere': [\n",
    "        'Ich erinnere mich gut',\n",
    "        'ich erinnere mich nicht',\n",
    "        'nochmal erinnere ich mich nicht'\n",
    "    ],\n",
    "    'erinnert': [\n",
    "        'wie erinnert man sich nochmal',\n",
    "        'wo erinnert man sich nochmal',\n",
    "        'vielleicht erinnert man sich dann nochmal'\n",
    "    ]\n",
    "}\n",
    "\n",
    "extractor.initialize(sentences_dict, overlap_threshold=0.4)\n",
    "extractor.print_trees()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sentences_dict = {\n",
    "#     'erinnere': [\n",
    "#         'Ich erinnere mich gut',\n",
    "#         'ich erinnere mich nicht',\n",
    "#         'nochmal erinnere ich mich nicht',\n",
    "#         'ich erinnere mich an das Treffen gestern',\n",
    "#         'erinnere mich bitte daran, morgen früh aufzustehen',\n",
    "#         'ich erinnere mich an die schöne Zeit'\n",
    "#     ],\n",
    "#     'erinnert': [\n",
    "#         'wie erinnert man sich nochmal',\n",
    "#         'wo erinnert man sich nochmal',\n",
    "#         'vielleicht erinnert man sich dann nochmal',\n",
    "#         'erinnert mich an meine Kindheit',\n",
    "#         'sie erinnert sich nicht mehr an das Gespräch',\n",
    "#         'er erinnert sich nicht gerne an die Vergangenheit'\n",
    "#     ],\n",
    "#     'erinnerte': [\n",
    "#         'er erinnerte sich plötzlich an den Vorfall',\n",
    "#         'ich erinnerte mich an mein erstes Auto',\n",
    "#         'sie erinnerte sich, dass sie etwas vergessen hatte',\n",
    "#         'erinnerte ich mich an den alten Freund',\n",
    "#         'er erinnerte sich an die Worte seiner Mutter',\n",
    "#         'ich erinnerte mich an den letzten Urlaub'\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# Build the trees from the sentences\n",
    "# extractor.create_tree_mask_as_root(sentences_dict)\n",
    "\n",
    "\n",
    "extractor.initialize(sentences_dict, overlap_threshold=0.4)\n",
    "\n",
    "extractor.print_trees()\n",
    "\n",
    "# # Initialize embeddings\n",
    "# extractor.initialize_node_embeddings(embedding_size=500)\n",
    "# extractor.id_to_node\n",
    "# # Step 1: Generate embeddings for all sentences\n",
    "\n",
    "# for weighting_scheme in ['no distance weighting', 'linear', 'inverse_sigmoid']:\n",
    "#     sentence_embeddings, sentence_list = extractor.get_all_sentence_embeddings(sentences_dict, weighting_scheme=weighting_scheme)\n",
    "#     n_clusters = 3  # You can adjust this based on the number of clusters you want\n",
    "#     clusters = extractor.cluster_sentences(sentence_embeddings, n_clusters=n_clusters)\n",
    "#     reduced_embeddings = extractor.reduce_dimensionality(sentence_embeddings)\n",
    "#     extractor.visualize_clusters(reduced_embeddings, clusters, sentence_list, title_appendix=weighting_scheme)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
