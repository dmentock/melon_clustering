{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.0010004043579101562\n",
      "2 0.03324413299560547\n",
      "3 0.0562436580657959\n",
      "4 0.0567474365234375\n",
      "5 0.0567474365234375\n",
      "6 0.583747148513794\n",
      "DBManager initializing...\n",
      "2024-09-01 19:24:26,567 DEBUG Initializing DBManager\n",
      "2024-09-01 19:24:26,703 DEBUG DBManager initialized successfully\n",
      "DBManager initializing...\n",
      "2024-09-01 19:24:28,348 DEBUG Initializing DBManager\n",
      "2024-09-01 19:24:28,478 DEBUG DBManager initialized successfully\n",
      "DBManager initializing...\n",
      "2024-09-01 19:24:28,695 DEBUG Initializing DBManager\n",
      "2024-09-01 19:24:28,817 DEBUG DBManager initialized successfully\n"
     ]
    }
   ],
   "source": [
    "from melon_db import Parser\n",
    "from melon_clustering import SENTENCES_DIR\n",
    "\n",
    "words = ['fühlen', 'ärgern', 'erinnern']\n",
    "\n",
    "for word in words:\n",
    "    lemmas = [word]\n",
    "    lang =  'de'\n",
    "    parser = Parser(lang)\n",
    "\n",
    "    res = parser.get_query(\n",
    "        \"\"\"\n",
    "        WITH RankedSegments AS (\n",
    "            SELECT l.lemma, ts.text, w.word, sn.name AS source_name, se.name AS source_entry_name, ts.index,\n",
    "                    ROW_NUMBER() OVER (PARTITION BY l.lemma, sn.name, sn.domain_subcategory_id ORDER BY se.index, ts.index) AS rn\n",
    "            FROM text_segments ts\n",
    "            JOIN source_entries se ON ts.source_entry_id = se.id\n",
    "            JOIN words_in_text_segments wits ON ts.id = wits.text_segment_id\n",
    "            JOIN words w ON wits.word_id = w.id\n",
    "            JOIN lemmas l ON w.lemma_id = l.id\n",
    "            JOIN source_names sn ON se.source_name_id = sn.id\n",
    "            WHERE l.lemma = %s AND sn.lang = %s)\n",
    "        SELECT word, text\n",
    "        FROM RankedSegments\n",
    "        ORDER BY lemma, source_entry_name, index;\"\"\", (word, lang)\n",
    "    )\n",
    "    result = {}\n",
    "    for a,i in res:\n",
    "        a = a.lower()\n",
    "        if a not in result:\n",
    "            result[a] = []\n",
    "        result[a].append(i.replace('\\r', ' ').replace('\\u2005', ' ').replace('  ', ' ').replace('  ', ' ').replace('  ', ' ').replace(' \\'', '\\''))\n",
    "    import yaml\n",
    "    with open(SENTENCES_DIR / f'{word}.yaml', 'w') as f:\n",
    "        yaml.dump(result, f)\n",
    "# import cluster\n",
    "\n",
    "# c = cluster.Cluster(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "# c.cluster_sentences(res)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
