{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.2094932794570923\n",
      "Epoch 10, Loss: 0.614757239818573\n",
      "Epoch 20, Loss: 0.3901924788951874\n",
      "Epoch 30, Loss: 0.278777152299881\n",
      "Epoch 40, Loss: 0.20645418763160706\n",
      "Epoch 50, Loss: 0.15519927442073822\n",
      "Epoch 60, Loss: 0.11905073374509811\n",
      "Epoch 70, Loss: 0.09134751558303833\n",
      "Epoch 80, Loss: 0.06781971454620361\n",
      "Epoch 90, Loss: 0.04919823631644249\n",
      "Epoch 100, Loss: 0.03684870898723602\n",
      "Epoch 110, Loss: 0.029125932604074478\n",
      "Epoch 120, Loss: 0.023681681603193283\n",
      "Epoch 130, Loss: 0.018271081149578094\n",
      "Epoch 140, Loss: 0.014003713615238667\n",
      "Epoch 150, Loss: 0.010769546963274479\n",
      "Epoch 160, Loss: 0.006648752838373184\n",
      "Epoch 170, Loss: 0.003942565992474556\n",
      "Epoch 180, Loss: 0.005508058704435825\n",
      "Epoch 190, Loss: 0.001960051478818059\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 215\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m morphology, sentences \u001b[38;5;129;01min\u001b[39;00m sentences_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[1;32m--> 215\u001b[0m         embedding \u001b[38;5;241m=\u001b[39m \u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_sentence_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgnn_node_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmorphology\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m         sentence_embeddings\u001b[38;5;241m.\u001b[39mappend(embedding)\n\u001b[0;32m    217\u001b[0m         sentence_list\u001b[38;5;241m.\u001b[39mappend(sentence)\n",
      "Cell \u001b[1;32mIn[2], line 138\u001b[0m, in \u001b[0;36mPatternExtractorWithGNN.generate_sentence_embeddings\u001b[1;34m(self, gnn_node_embeddings, sentence, morphology, steepness)\u001b[0m\n\u001b[0;32m    135\u001b[0m         current_node \u001b[38;5;241m=\u001b[39m current_node\u001b[38;5;241m.\u001b[39mchildren[word]\n\u001b[0;32m    136\u001b[0m         node_path\u001b[38;5;241m.\u001b[39mappend(current_node\u001b[38;5;241m.\u001b[39mid)\n\u001b[1;32m--> 138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_weighted_sentence_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgnn_node_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteepness\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteepness\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 144\u001b[0m, in \u001b[0;36mPatternExtractorWithGNN.compute_weighted_sentence_embedding\u001b[1;34m(self, gnn_node_embeddings, sentence_path, steepness)\u001b[0m\n\u001b[0;32m    142\u001b[0m weighted_embedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(gnn_node_embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, node_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sentence_path):\n\u001b[1;32m--> 144\u001b[0m     weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241;43m-\u001b[39;49m\u001b[43msteepness\u001b[49m \u001b[38;5;241m*\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))))\n\u001b[0;32m    145\u001b[0m     weighted_embedding \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m weight \u001b[38;5;241m*\u001b[39m gnn_node_embeddings[node_id]\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weighted_embedding \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(sentence_path)\n",
      "\u001b[1;31mTypeError\u001b[0m: bad operand type for unary -: 'NoneType'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from melon_clustering import PatternExtractor\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # First GCN layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        # Second GCN layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class PatternExtractorWithGNN(PatternExtractor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.graph = nx.DiGraph()  # Graph to store word relations\n",
    "        self.node_embeddings = {}  # To store GCN embeddings\n",
    "\n",
    "    def initialize_node_features(self, feature_dim=100):\n",
    "        # Randomly initialize node features\n",
    "        num_nodes = self.node_counter\n",
    "        node_features = torch.randn((num_nodes, feature_dim), requires_grad=True)\n",
    "        return node_features\n",
    "\n",
    "    def train_gnn(self, edge_index, node_features, hidden_dim=64, output_dim=100, epochs=200):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = GCN(input_dim=node_features.shape[1], hidden_dim=hidden_dim, output_dim=output_dim).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        # Move data to device\n",
    "        edge_index = edge_index.to(device)\n",
    "        node_features = node_features.to(device)\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            out = model(node_features, edge_index)\n",
    "            loss = F.mse_loss(out, node_features)  # Unsupervised loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "        return out.detach().cpu().numpy()\n",
    "\n",
    "    def build_graph(self, sentences_dict):\n",
    "        self.create_tree_mask_as_root(sentences_dict)\n",
    "        edge_index = torch.tensor(list(self.graph.edges)).t().contiguous()\n",
    "        return edge_index\n",
    "\n",
    "    def generate_sentence_embeddings(self, gnn_node_embeddings, sentence, morphology, steepness=None):\n",
    "        words = sentence.lower().split()\n",
    "        node_path = deque([0])\n",
    "        key_word_index = words.index(morphology.lower())\n",
    "        words_before = words[:key_word_index]\n",
    "        words_after = words[key_word_index + 1:]\n",
    "\n",
    "        current_node = self.preceding_tree\n",
    "        for word in words_before[::-1]:\n",
    "            if word in current_node.children:\n",
    "                current_node = current_node.children[word]\n",
    "                node_path.appendleft(current_node.id)\n",
    "\n",
    "        current_node = self.following_tree\n",
    "        for word in words_after:\n",
    "            if word in current_node.children:\n",
    "                current_node = current_node.children[word]\n",
    "                node_path.append(current_node.id)\n",
    "\n",
    "        return self.compute_weighted_sentence_embedding(gnn_node_embeddings, node_path, steepness=steepness)\n",
    "\n",
    "    def compute_weighted_sentence_embedding(self, gnn_node_embeddings, sentence_path, steepness=None):\n",
    "        weighted_embedding = np.zeros(gnn_node_embeddings.shape[1])\n",
    "        for i, node_id in enumerate(sentence_path):\n",
    "            weight = 1 - (1 / (1 + np.exp(-steepness * (i + 1))))\n",
    "            weighted_embedding += weight * gnn_node_embeddings[node_id]\n",
    "        return weighted_embedding / len(sentence_path)\n",
    "\n",
    "    def cluster_embeddings(self, embeddings, n_clusters=3):\n",
    "        \"\"\"\n",
    "        Apply K-Means clustering to the GNN-generated embeddings.\n",
    "        \"\"\"\n",
    "        kmeans = KMeans(n_clusters=n_clusters)\n",
    "        clusters = kmeans.fit_predict(embeddings)\n",
    "        return clusters\n",
    "\n",
    "    def reduce_dimensionality(self, embeddings, method='pca', n_components=2):\n",
    "        \"\"\"\n",
    "        Reduce dimensionality of embeddings for visualization using PCA, t-SNE.\n",
    "        \"\"\"\n",
    "        if method == 'pca':\n",
    "            reducer = PCA(n_components=n_components)\n",
    "        elif method == 'tsne':\n",
    "            perplexity = min(30, len(embeddings) - 1)  # Ensure perplexity is smaller than the number of samples\n",
    "            reducer = TSNE(n_components=n_components, perplexity=perplexity)\n",
    "\n",
    "        reduced_embeddings = reducer.fit_transform(embeddings)\n",
    "        return reduced_embeddings\n",
    "\n",
    "    def visualize_clusters(self, reduced_embeddings, clusters, sentence_list=None, method='pca', appendix = None):\n",
    "        \"\"\"\n",
    "        Visualize clusters using a 2D scatter plot.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(15, 15))\n",
    "        scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=clusters, cmap='viridis')\n",
    "\n",
    "        # Annotate sentences for better interpretability\n",
    "        if sentence_list:\n",
    "            for i, sentence in enumerate(sentence_list):\n",
    "                plt.annotate(sentence, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]), fontsize=8, alpha=0.7)\n",
    "\n",
    "        plt.title(f\"Sentence Clustering Visualization ({method}{' ' + str(appendix) if appendix else ''})\")\n",
    "        plt.xlabel(f\"{method.upper()} Component 1\")\n",
    "        plt.ylabel(f\"{method.upper()} Component 2\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def print_central_and_edge_sentences(sentence_embeddings, clusters, sentence_list, n=3):\n",
    "    \"\"\"\n",
    "    Prints n central and n edge sentences for each cluster.\n",
    "    \"\"\"\n",
    "    # Get the centroids from the kmeans clustering\n",
    "    kmeans = KMeans(n_clusters=len(np.unique(clusters)))\n",
    "    kmeans.fit(sentence_embeddings)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "\n",
    "    for cluster_id in range(len(centroids)):\n",
    "        # Get the indices of sentences in this cluster\n",
    "        cluster_indices = [i for i, cluster in enumerate(clusters) if cluster == cluster_id]\n",
    "        cluster_sentences = [sentence_list[i] for i in cluster_indices]\n",
    "        cluster_embeddings = sentence_embeddings[cluster_indices]\n",
    "\n",
    "        # Compute the distance of each sentence in the cluster to the centroid\n",
    "        distances = np.linalg.norm(cluster_embeddings - centroids[cluster_id], axis=1)\n",
    "\n",
    "        # Get indices of the n closest and n farthest sentences\n",
    "        closest_indices = np.argsort(distances)[:n]\n",
    "        farthest_indices = np.argsort(distances)[-n:]\n",
    "\n",
    "        print(f\"\\nCluster {cluster_id + 1}:\")\n",
    "        print(f\"Centroid: {centroids[cluster_id]}\")\n",
    "\n",
    "        print(\"\\n  Central Sentences (closest to the centroid):\")\n",
    "        for idx in closest_indices:\n",
    "            print(f\"  - {cluster_sentences[idx]}\")\n",
    "\n",
    "        print(\"\\n  Edge Sentences (farthest from the centroid):\")\n",
    "        for idx in farthest_indices:\n",
    "            print(f\"  - {cluster_sentences[idx]}\")\n",
    "\n",
    "\n",
    "# Create an instance of the PatternExtractorWithGNN\n",
    "extractor = PatternExtractorWithGNN()\n",
    "\n",
    "# Step 1: Build the graph from sentences\n",
    "sentences_dict = {\n",
    "    # 'erinnere': [\n",
    "    #     'Ich erinnere mich gut',\n",
    "    #     'ich erinnere mich nicht',\n",
    "    #     'nochmal erinnere ich mich nicht',\n",
    "    #     'ich erinnere mich an <ARTICLE> Treffen gestern',\n",
    "    #     'erinnere mich bitte daran, morgen früh aufzustehen',\n",
    "    #     'ich erinnere mich an <ARTICLE> schöne Zeit'\n",
    "    # ],\n",
    "    'erinnert': [\n",
    "        # 'wie erinnert man sich nochmal',\n",
    "        # 'wo erinnert man sich nochmal',\n",
    "        # 'vielleicht erinnert man sich dann nochmal',\n",
    "        # 'erinnert mich an meine Kindheit',\n",
    "        'sie erinnert sich nicht mehr an <ARTICLE> Gespräch',\n",
    "        'er erinnert sich nicht gerne an <ARTICLE> Vergangenheit'\n",
    "    ],\n",
    "    # 'erinnerte': [\n",
    "    #     'er erinnerte sich plötzlich an den Vorfall',\n",
    "    #     'ich erinnerte mich an mein erstes Auto',\n",
    "    #     'sie erinnerte sich, dass sie etwas vergessen hatte',\n",
    "    #     'erinnerte ich mich an den alten Freund',\n",
    "    #     'er erinnerte sich an <ARTICLE> Worte seiner Mutter',\n",
    "    #     'ich erinnerte mich an den letzten Urlaub'\n",
    "    # ]\n",
    "}\n",
    "\n",
    "# language = 'de'\n",
    "# from melon_clustering import load_sentences, SENTENCES_DIR\n",
    "# sentences_dict = load_sentences(SENTENCES_DIR / 'erinnern.yaml', language)\n",
    "\n",
    "extractor.initialize(sentences_dict, overlap_threshold = 0.4)\n",
    "\n",
    "# Build graph from the sentences\n",
    "edge_index = extractor.build_graph(sentences_dict)\n",
    "\n",
    "# Step 2: Initialize node features\n",
    "node_features = extractor.initialize_node_features(feature_dim=100)\n",
    "\n",
    "# Step 3: Train GNN and get embeddings\n",
    "gnn_node_embeddings = extractor.train_gnn(edge_index, node_features)\n",
    "\n",
    "# Rest of your code\n",
    "for steepness in [0.5, 1, 2, 3]:\n",
    "    sentence_embeddings = []\n",
    "    sentence_list = []\n",
    "    for morphology, sentences in sentences_dict.items():\n",
    "        for sentence in sentences:\n",
    "            embedding = extractor.generate_sentence_embeddings(gnn_node_embeddings, sentence, morphology, steepness=steepness)\n",
    "            sentence_embeddings.append(embedding)\n",
    "            sentence_list.append(sentence)\n",
    "\n",
    "    # Convert the list of embeddings to a NumPy array\n",
    "    sentence_embeddings = np.array(sentence_embeddings)\n",
    "\n",
    "    # Step 5: Cluster the embeddings\n",
    "    clusters = extractor.cluster_embeddings(sentence_embeddings, n_clusters=3)\n",
    "\n",
    "    # Step 6: Apply dimensionality reduction methods (PCA, t-SNE)\n",
    "    for method in ['pca', 'tsne']:\n",
    "        reduced_embeddings = extractor.reduce_dimensionality(sentence_embeddings, method=method)\n",
    "\n",
    "        # Step 7: Visualize clusters using different methods\n",
    "        extractor.visualize_clusters(reduced_embeddings, clusters, method=method, appendix=steepness)\n",
    "\n",
    "    # Step 8: Print 3 central and 3 edge sentences for each cluster\n",
    "    print_central_and_edge_sentences(sentence_embeddings, clusters, sentence_list, n=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
