{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.267358422279358\n",
      "Epoch 10, Loss: 0.8175414204597473\n",
      "Epoch 20, Loss: 0.6404873132705688\n",
      "Epoch 30, Loss: 0.5272635221481323\n",
      "Epoch 40, Loss: 0.4466923475265503\n",
      "Epoch 50, Loss: 0.3831380307674408\n",
      "Epoch 60, Loss: 0.33280423283576965\n",
      "Epoch 70, Loss: 0.2949002981185913\n",
      "Epoch 80, Loss: 0.2658277750015259\n",
      "Epoch 90, Loss: 0.24245136976242065\n",
      "Epoch 100, Loss: 0.22310157120227814\n",
      "Epoch 110, Loss: 0.206974059343338\n",
      "Epoch 120, Loss: 0.1937795877456665\n",
      "Epoch 130, Loss: 0.18225686252117157\n",
      "Epoch 140, Loss: 0.17241311073303223\n",
      "Epoch 150, Loss: 0.16513165831565857\n",
      "Epoch 160, Loss: 0.15646789968013763\n",
      "Epoch 170, Loss: 0.14908599853515625\n",
      "Epoch 180, Loss: 0.14688155055046082\n",
      "Epoch 190, Loss: 0.13852299749851227\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 273\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m morphology, sentences \u001b[38;5;129;01min\u001b[39;00m sentences_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[1;32m--> 273\u001b[0m         embedding \u001b[38;5;241m=\u001b[39m \u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_sentence_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgnn_node_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmorphology\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweighting_scheme\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweighting_scheme\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m         sentence_embeddings\u001b[38;5;241m.\u001b[39mappend(embedding)\n\u001b[0;32m    275\u001b[0m         sentence_list\u001b[38;5;241m.\u001b[39mappend(sentence)\n",
      "Cell \u001b[1;32mIn[8], line 168\u001b[0m, in \u001b[0;36mPatternExtractorWithGNN.generate_sentence_embeddings\u001b[1;34m(self, gnn_node_embeddings, sentence, morphology, weighting_scheme)\u001b[0m\n\u001b[0;32m    165\u001b[0m         node_path\u001b[38;5;241m.\u001b[39mappend(current_node\u001b[38;5;241m.\u001b[39mid)\n\u001b[0;32m    166\u001b[0m         word_vectors\u001b[38;5;241m.\u001b[39mappend(gnn_node_embeddings[current_node\u001b[38;5;241m.\u001b[39mid])\n\u001b[1;32m--> 168\u001b[0m word_vectors \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_vectors\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Convert list to tensor\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# Use attention mechanism to compute sentence embedding\u001b[39;00m\n\u001b[0;32m    171\u001b[0m sentence_embedding, attention_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(word_vectors)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got numpy.ndarray"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention_weights = nn.Linear(input_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, word_vectors):\n",
    "        \"\"\"\n",
    "        Applies attention to word vectors and returns a weighted sum.\n",
    "        word_vectors: Tensor of shape (num_words, embedding_dim)\n",
    "        \"\"\"\n",
    "        scores = self.attention_weights(word_vectors)  # Compute attention scores\n",
    "        weights = torch.softmax(scores, dim=0)  # Normalize scores to get weights\n",
    "        weighted_sum = torch.sum(weights * word_vectors, dim=0)  # Weighted sum of word vectors\n",
    "        return weighted_sum, weights\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # First GCN layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        # Second GCN layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, word, id):\n",
    "        self.word = word\n",
    "        self.id = id\n",
    "        self.count = 0\n",
    "        self.children = {}\n",
    "\n",
    "class PatternExtractorWithGNN:\n",
    "    def __init__(self):\n",
    "        self.preceding_tree = Node('<ROOT>', 0)\n",
    "        self.following_tree = Node('<ROOT>', 1)\n",
    "        self.node_counter = 2\n",
    "        self.word_to_ids = defaultdict(list)\n",
    "        self.id_to_node = {0: self.preceding_tree, 1: self.following_tree}\n",
    "        self.child_to_parents = defaultdict(list)\n",
    "        self.graph = nx.DiGraph()  # Graph to store word relations\n",
    "        self.node_embeddings = {}  # To store GCN embeddings\n",
    "        self.attention = Attention(100)  # Assuming 100 is the embedding size\n",
    "\n",
    "    def add_start_end_flags_lower(self, sentences):\n",
    "        return [f\"<START> {sentence.lower()} <END>\" for sentence in sentences]\n",
    "\n",
    "    def get_or_create_node(self, current_tree, word, parent_id=None):\n",
    "        if word not in current_tree.children:\n",
    "            new_node = Node(word, self.node_counter)\n",
    "            current_tree.children[word] = new_node\n",
    "            self.word_to_ids[word].append(self.node_counter)\n",
    "            self.id_to_node[self.node_counter] = new_node\n",
    "            if parent_id is not None:\n",
    "                self.child_to_parents[self.node_counter].append(parent_id)\n",
    "                self.graph.add_edge(parent_id, self.node_counter)  # Add edge in the graph\n",
    "            self.node_counter += 1\n",
    "        else:\n",
    "            child_id = current_tree.children[word].id\n",
    "            if parent_id is not None:\n",
    "                self.child_to_parents[child_id].append(parent_id)\n",
    "                self.graph.add_edge(parent_id, child_id)\n",
    "        return current_tree.children[word]\n",
    "\n",
    "    def add_to_tree(self, words, direction):\n",
    "        current_tree = self.preceding_tree if direction == 'preceding' else self.following_tree\n",
    "        word_range = range(len(words) - 1, -1, -1) if direction == 'preceding' else range(len(words))\n",
    "        parent_id = current_tree.id\n",
    "\n",
    "        for i in word_range:\n",
    "            current_word = words[i]\n",
    "            current_tree = self.get_or_create_node(current_tree, current_word, parent_id)\n",
    "            parent_id = current_tree.id\n",
    "\n",
    "    def create_tree_mask_as_root(self, sentences_dict):\n",
    "        for morphology, sentences in sentences_dict.items():\n",
    "            sentences_with_flags = self.add_start_end_flags_lower(sentences)\n",
    "\n",
    "            for sentence in sentences_with_flags:\n",
    "                words = sentence.lower().split()\n",
    "                key_word_index = words.index(morphology.lower())\n",
    "                words_before = words[:key_word_index]\n",
    "                words_after = words[key_word_index + 1:]\n",
    "\n",
    "                self.add_to_tree(words_before, 'preceding')\n",
    "                self.add_to_tree(words_after, 'following')\n",
    "\n",
    "    def build_graph(self, sentences_dict):\n",
    "        self.create_tree_mask_as_root(sentences_dict)\n",
    "        edge_index = torch.tensor(list(self.graph.edges)).t().contiguous()\n",
    "        return edge_index\n",
    "\n",
    "    def initialize_node_features(self, feature_dim=100):\n",
    "        # Randomly initialize node features\n",
    "        num_nodes = self.node_counter\n",
    "        node_features = torch.randn((num_nodes, feature_dim), requires_grad=True)\n",
    "        return node_features\n",
    "\n",
    "    def train_gnn(self, edge_index, node_features, hidden_dim=64, output_dim=100, epochs=200):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = GCN(input_dim=node_features.shape[1], hidden_dim=hidden_dim, output_dim=output_dim).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        # Move data to device\n",
    "        edge_index = edge_index.to(device)\n",
    "        node_features = node_features.to(device)\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            out = model(node_features, edge_index)\n",
    "            loss = F.mse_loss(out, node_features)  # Unsupervised loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "        return out.detach().cpu().numpy()\n",
    "\n",
    "    def generate_sentence_embeddings(self, gnn_node_embeddings, sentence, morphology, weighting_scheme=None):\n",
    "        words = sentence.lower().split()\n",
    "        node_path = deque([0])\n",
    "        key_word_index = words.index(morphology.lower())\n",
    "        words_before = words[:key_word_index]\n",
    "        words_after = words[key_word_index + 1:]\n",
    "\n",
    "        word_vectors = []\n",
    "\n",
    "        # Get word vectors for the preceding words\n",
    "        current_node = self.preceding_tree\n",
    "        for word in words_before[::-1]:\n",
    "            if word in current_node.children:\n",
    "                current_node = current_node.children[word]\n",
    "                node_path.appendleft(current_node.id)\n",
    "                word_vectors.append(gnn_node_embeddings[current_node.id])\n",
    "\n",
    "        # Get word vectors for the following words\n",
    "        current_node = self.following_tree\n",
    "        for word in words_after:\n",
    "            if word in current_node.children:\n",
    "                current_node = current_node.children[word]\n",
    "                node_path.append(current_node.id)\n",
    "                word_vectors.append(gnn_node_embeddings[current_node.id])\n",
    "\n",
    "        word_vectors = torch.stack(word_vectors)  # Convert list to tensor\n",
    "\n",
    "        # Use attention mechanism to compute sentence embedding\n",
    "        sentence_embedding, attention_weights = self.attention(word_vectors)\n",
    "\n",
    "        return sentence_embedding\n",
    "\n",
    "    def compute_weighted_sentence_embedding(self, gnn_node_embeddings, sentence_path, weighting_scheme=None):\n",
    "        weighted_embedding = np.zeros(gnn_node_embeddings.shape[1])\n",
    "        for i, node_id in enumerate(sentence_path):\n",
    "            if weighting_scheme == 'linear':\n",
    "                weight = 1 / (i + 1)\n",
    "            elif weighting_scheme == 'inverse_sigmoid':\n",
    "                steepness = 0.2\n",
    "                weight = 1 - (1 / (1 + np.exp(-steepness * (i + 1))))\n",
    "            else:\n",
    "                weight = 1\n",
    "            weighted_embedding += weight * gnn_node_embeddings[node_id]\n",
    "        return weighted_embedding / len(sentence_path)\n",
    "\n",
    "    def cluster_embeddings(self, embeddings, n_clusters=3):\n",
    "        \"\"\"\n",
    "        Apply K-Means clustering to the GNN-generated embeddings.\n",
    "        \"\"\"\n",
    "        kmeans = KMeans(n_clusters=n_clusters)\n",
    "        clusters = kmeans.fit_predict(embeddings)\n",
    "        return clusters\n",
    "\n",
    "    def reduce_dimensionality(self, embeddings, method='pca', n_components=2):\n",
    "        \"\"\"\n",
    "        Reduce dimensionality of embeddings for visualization using PCA, t-SNE, or UMAP.\n",
    "        \"\"\"\n",
    "        if method == 'pca':\n",
    "            reducer = PCA(n_components=n_components)\n",
    "        elif method == 'tsne':\n",
    "            perplexity = min(30, len(embeddings) - 1)  # Ensure perplexity is smaller than the number of samples\n",
    "            reducer = TSNE(n_components=n_components, perplexity=perplexity)\n",
    "        elif method == 'umap':\n",
    "            reducer = umap.UMAP(n_components=n_components)\n",
    "\n",
    "        reduced_embeddings = reducer.fit_transform(embeddings)\n",
    "        return reduced_embeddings\n",
    "\n",
    "    def visualize_clusters(self, reduced_embeddings, clusters, sentence_list, method='pca'):\n",
    "        \"\"\"\n",
    "        Visualize clusters using a 2D scatter plot.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=clusters, cmap='viridis')\n",
    "\n",
    "        # Annotate sentences for better interpretability\n",
    "        for i, sentence in enumerate(sentence_list):\n",
    "            plt.annotate(sentence, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]), fontsize=8, alpha=0.7)\n",
    "\n",
    "        plt.title(f\"Sentence Clustering Visualization ({method})\")\n",
    "        plt.xlabel(f\"{method.upper()} Component 1\")\n",
    "        plt.ylabel(f\"{method.upper()} Component 2\")\n",
    "        plt.show()\n",
    "\n",
    "# Create an instance of the PatternExtractorWithGNN\n",
    "extractor = PatternExtractorWithGNN()\n",
    "\n",
    "# Step 1: Build the graph from sentences\n",
    "sentences_dict = {\n",
    "    'erinnere': [\n",
    "        'Ich erinnere mich gut',\n",
    "        'ich erinnere mich nicht',\n",
    "        'nochmal erinnere ich mich nicht',\n",
    "        'ich erinnere mich an das Treffen gestern',\n",
    "        'erinnere mich bitte daran, morgen früh aufzustehen',\n",
    "        'ich erinnere mich an die schöne Zeit'\n",
    "    ],\n",
    "    'erinnert': [\n",
    "        'wie erinnert man sich nochmal',\n",
    "        'wo erinnert man sich nochmal',\n",
    "        'vielleicht erinnert man sich dann nochmal',\n",
    "        'erinnert mich an meine Kindheit',\n",
    "        'sie erinnert sich nicht mehr an das Gespräch',\n",
    "        'er erinnert sich nicht gerne an die Vergangenheit'\n",
    "    ],\n",
    "    'erinnerte': [\n",
    "        'er erinnerte sich plötzlich an den Vorfall',\n",
    "        'ich erinnerte mich an mein erstes Auto',\n",
    "        'sie erinnerte sich, dass sie etwas vergessen hatte',\n",
    "        'erinnerte ich mich an den alten Freund',\n",
    "        'er erinnerte sich an die Worte seiner Mutter',\n",
    "        'ich erinnerte mich an den letzten Urlaub'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Build graph from the sentences\n",
    "edge_index = extractor.build_graph(sentences_dict)\n",
    "\n",
    "# Step 2: Initialize node features\n",
    "node_features = extractor.initialize_node_features(feature_dim=100)\n",
    "\n",
    "# Step 3: Train GNN and get embeddings\n",
    "gnn_node_embeddings = extractor.train_gnn(edge_index, node_features)\n",
    "\n",
    "# Step 4: Get embeddings for all sentences using different weighting schemes\n",
    "for weighting_scheme in ['no weighting', 'linear', 'inverse_sigmoid']:\n",
    "    sentence_embeddings = []\n",
    "    sentence_list = []\n",
    "    for morphology, sentences in sentences_dict.items():\n",
    "        for sentence in sentences:\n",
    "            embedding = extractor.generate_sentence_embeddings(gnn_node_embeddings, sentence, morphology, weighting_scheme=weighting_scheme)\n",
    "            sentence_embeddings.append(embedding)\n",
    "            sentence_list.append(sentence)\n",
    "\n",
    "    sentence_embeddings = np.array(sentence_embeddings)\n",
    "\n",
    "    # Step 5: Cluster the embeddings\n",
    "    clusters = extractor.cluster_embeddings(sentence_embeddings, n_clusters=3)\n",
    "\n",
    "    # Step 6: Try different dimensionality reduction methods (PCA, t-SNE, UMAP)\n",
    "    for method in ['pca', 'tsne', 'umap']:\n",
    "        reduced_embeddings = extractor.reduce_dimensionality(sentence_embeddings, method=method)\n",
    "\n",
    "        # Step 7: Visualize clusters using different methods\n",
    "        extractor.visualize_clusters(reduced_embeddings, clusters, sentence_list, method=method)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
