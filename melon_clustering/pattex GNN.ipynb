{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimize <START>\n",
      "optimize <END>\n",
      "Preceding Tree (before <MASK>):\n",
      "<ROOT> (count: 0, id: 0)\n",
      "  ich (count: 6, id: 2)\n",
      "    <start> (count: 6, id: 3)\n",
      "  nochmal (count: 1, id: 9)\n",
      "    <start> (count: 1, id: 10)\n",
      "  <start> (count: 3, id: 20)\n",
      "  wie (count: 1, id: 31)\n",
      "    <start> (count: 1, id: 32)\n",
      "  wo (count: 1, id: 37)\n",
      "    <start> (count: 1, id: 38)\n",
      "  vielleicht (count: 1, id: 39)\n",
      "    <start> (count: 1, id: 40)\n",
      "  sie (count: 2, id: 47)\n",
      "    <start> (count: 2, id: 48)\n",
      "  er (count: 3, id: 56)\n",
      "    <start> (count: 3, id: 57)\n",
      "\n",
      "Following Tree (after <MASK>):\n",
      "<ROOT> (count: 0, id: 1)\n",
      "  mich (count: 8, id: 4)\n",
      "    gut (count: 1, id: 5)\n",
      "      <end> (count: 1, id: 6)\n",
      "    nicht (count: 1, id: 7)\n",
      "      <end> (count: 1, id: 8)\n",
      "    an (count: 5, id: 15)\n",
      "      das (count: 1, id: 16)\n",
      "        treffen (count: 1, id: 17)\n",
      "          gestern (count: 1, id: 18)\n",
      "            <end> (count: 1, id: 19)\n",
      "      die (count: 1, id: 27)\n",
      "        schöne (count: 1, id: 28)\n",
      "          zeit (count: 1, id: 29)\n",
      "            <end> (count: 1, id: 30)\n",
      "      meine (count: 1, id: 44)\n",
      "        kindheit (count: 1, id: 45)\n",
      "          <end> (count: 1, id: 46)\n",
      "      mein (count: 1, id: 68)\n",
      "        erstes (count: 1, id: 69)\n",
      "          auto (count: 1, id: 70)\n",
      "            <end> (count: 1, id: 71)\n",
      "      den (count: 1, id: 90)\n",
      "        letzten (count: 1, id: 91)\n",
      "          urlaub (count: 1, id: 92)\n",
      "            <end> (count: 1, id: 93)\n",
      "    bitte (count: 1, id: 21)\n",
      "      daran, (count: 1, id: 22)\n",
      "        morgen (count: 1, id: 23)\n",
      "          früh (count: 1, id: 24)\n",
      "            aufzustehen (count: 1, id: 25)\n",
      "              <end> (count: 1, id: 26)\n",
      "  ich (count: 2, id: 11)\n",
      "    mich (count: 2, id: 12)\n",
      "      nicht (count: 1, id: 13)\n",
      "        <end> (count: 1, id: 14)\n",
      "      an (count: 1, id: 79)\n",
      "        den (count: 1, id: 80)\n",
      "          alten (count: 1, id: 81)\n",
      "            freund (count: 1, id: 82)\n",
      "              <end> (count: 1, id: 83)\n",
      "  man (count: 3, id: 33)\n",
      "    sich (count: 3, id: 34)\n",
      "      nochmal (count: 2, id: 35)\n",
      "        <end> (count: 2, id: 36)\n",
      "      dann (count: 1, id: 41)\n",
      "        nochmal (count: 1, id: 42)\n",
      "          <end> (count: 1, id: 43)\n",
      "  sich (count: 4, id: 49)\n",
      "    nicht (count: 2, id: 50)\n",
      "      mehr (count: 1, id: 51)\n",
      "        an (count: 1, id: 52)\n",
      "          das (count: 1, id: 53)\n",
      "            gespräch (count: 1, id: 54)\n",
      "              <end> (count: 1, id: 55)\n",
      "      gerne (count: 1, id: 58)\n",
      "        an (count: 1, id: 59)\n",
      "          die (count: 1, id: 60)\n",
      "            vergangenheit (count: 1, id: 61)\n",
      "              <end> (count: 1, id: 62)\n",
      "    plötzlich (count: 1, id: 63)\n",
      "      an (count: 1, id: 64)\n",
      "        den (count: 1, id: 65)\n",
      "          vorfall (count: 1, id: 66)\n",
      "            <end> (count: 1, id: 67)\n",
      "    an (count: 1, id: 84)\n",
      "      die (count: 1, id: 85)\n",
      "        worte (count: 1, id: 86)\n",
      "          seiner (count: 1, id: 87)\n",
      "            mutter (count: 1, id: 88)\n",
      "              <end> (count: 1, id: 89)\n",
      "  sich, (count: 1, id: 72)\n",
      "    dass (count: 1, id: 73)\n",
      "      sie (count: 1, id: 74)\n",
      "        etwas (count: 1, id: 75)\n",
      "          vergessen (count: 1, id: 76)\n",
      "            hatte (count: 1, id: 77)\n",
      "              <end> (count: 1, id: 78)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from melon_clustering import PatternExtractor\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # First GCN layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        # Second GCN layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class PatternExtractorWithGNN(PatternExtractor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.graph = nx.DiGraph()  # Graph to store word relations\n",
    "        self.node_embeddings = {}  # To store GCN embeddings\n",
    "\n",
    "    def initialize_node_features(self, feature_dim=100):\n",
    "        # Randomly initialize node features\n",
    "        num_nodes = self.node_counter\n",
    "        node_features = torch.randn((num_nodes, feature_dim), requires_grad=True)\n",
    "        return node_features\n",
    "\n",
    "    def train_gnn(self, edge_index, node_features, hidden_dim=64, output_dim=100, epochs=200):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = GCN(input_dim=node_features.shape[1], hidden_dim=hidden_dim, output_dim=output_dim).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        # Move data to device\n",
    "        edge_index = edge_index.to(device)\n",
    "        node_features = node_features.to(device)\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            out = model(node_features, edge_index)\n",
    "            loss = F.mse_loss(out, node_features)  # Unsupervised loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "        return out.detach().cpu().numpy()\n",
    "\n",
    "    def generate_sentence_embeddings(self, gnn_node_embeddings, sentence, morphology, steepness=None):\n",
    "        words = sentence.lower().split()\n",
    "        node_path = deque([0])\n",
    "        key_word_index = words.index(morphology.lower())\n",
    "        words_before = words[:key_word_index]\n",
    "        words_after = words[key_word_index + 1:]\n",
    "\n",
    "        current_node = self.preceding_tree\n",
    "        for word in words_before[::-1]:\n",
    "            if word in current_node.children:\n",
    "                current_node = current_node.children[word]\n",
    "                node_path.appendleft(current_node.id)\n",
    "\n",
    "        current_node = self.following_tree\n",
    "        for word in words_after:\n",
    "            if word in current_node.children:\n",
    "                current_node = current_node.children[word]\n",
    "                node_path.append(current_node.id)\n",
    "\n",
    "        return self.compute_weighted_sentence_embedding(gnn_node_embeddings, node_path, steepness=steepness)\n",
    "\n",
    "    def compute_weighted_sentence_embedding(self, gnn_node_embeddings, sentence_path, steepness=None):\n",
    "        weighted_embedding = np.zeros(gnn_node_embeddings.shape[1])\n",
    "        for i, node_id in enumerate(sentence_path):\n",
    "            weight = 1 - (1 / (1 + np.exp(-steepness * (i + 1))))\n",
    "            weighted_embedding += weight * gnn_node_embeddings[node_id]\n",
    "        return weighted_embedding / len(sentence_path)\n",
    "\n",
    "    def cluster_embeddings(self, embeddings, n_clusters=3):\n",
    "        \"\"\"\n",
    "        Apply K-Means clustering to the GNN-generated embeddings.\n",
    "        \"\"\"\n",
    "        kmeans = KMeans(n_clusters=n_clusters)\n",
    "        clusters = kmeans.fit_predict(embeddings)\n",
    "        return clusters\n",
    "\n",
    "    def reduce_dimensionality(self, embeddings, method='pca', n_components=2):\n",
    "        \"\"\"\n",
    "        Reduce dimensionality of embeddings for visualization using PCA, t-SNE.\n",
    "        \"\"\"\n",
    "        if method == 'pca':\n",
    "            reducer = PCA(n_components=n_components)\n",
    "        elif method == 'tsne':\n",
    "            perplexity = min(30, len(embeddings) - 1)  # Ensure perplexity is smaller than the number of samples\n",
    "            reducer = TSNE(n_components=n_components, perplexity=perplexity)\n",
    "\n",
    "        reduced_embeddings = reducer.fit_transform(embeddings)\n",
    "        return reduced_embeddings\n",
    "\n",
    "    def visualize_clusters(self, reduced_embeddings, clusters, sentence_list=None, method='pca', appendix = None):\n",
    "        \"\"\"\n",
    "        Visualize clusters using a 2D scatter plot.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(15, 15))\n",
    "        scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=clusters, cmap='viridis')\n",
    "\n",
    "        # Annotate sentences for better interpretability\n",
    "        if sentence_list:\n",
    "            for i, sentence in enumerate(sentence_list):\n",
    "                plt.annotate(sentence, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]), fontsize=8, alpha=0.7)\n",
    "\n",
    "        plt.title(f\"Sentence Clustering Visualization ({method}{' ' + str(appendix) if appendix else ''})\")\n",
    "        plt.xlabel(f\"{method.upper()} Component 1\")\n",
    "        plt.ylabel(f\"{method.upper()} Component 2\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def print_central_and_edge_sentences(sentence_embeddings, clusters, sentence_list, n=3):\n",
    "    \"\"\"\n",
    "    Prints n central and n edge sentences for each cluster.\n",
    "    \"\"\"\n",
    "    # Get the centroids from the kmeans clustering\n",
    "    kmeans = KMeans(n_clusters=len(np.unique(clusters)))\n",
    "    kmeans.fit(sentence_embeddings)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "\n",
    "    for cluster_id in range(len(centroids)):\n",
    "        # Get the indices of sentences in this cluster\n",
    "        cluster_indices = [i for i, cluster in enumerate(clusters) if cluster == cluster_id]\n",
    "        cluster_sentences = [sentence_list[i] for i in cluster_indices]\n",
    "        cluster_embeddings = sentence_embeddings[cluster_indices]\n",
    "\n",
    "        # Compute the distance of each sentence in the cluster to the centroid\n",
    "        distances = np.linalg.norm(cluster_embeddings - centroids[cluster_id], axis=1)\n",
    "\n",
    "        # Get indices of the n closest and n farthest sentences\n",
    "        closest_indices = np.argsort(distances)[:n]\n",
    "        farthest_indices = np.argsort(distances)[-n:]\n",
    "\n",
    "        print(f\"\\nCluster {cluster_id + 1}:\")\n",
    "        print(f\"Centroid: {centroids[cluster_id]}\")\n",
    "\n",
    "        print(\"\\n  Central Sentences (closest to the centroid):\")\n",
    "        for idx in closest_indices:\n",
    "            print(f\"  - {cluster_sentences[idx]}\")\n",
    "\n",
    "        print(\"\\n  Edge Sentences (farthest from the centroid):\")\n",
    "        for idx in farthest_indices:\n",
    "            print(f\"  - {cluster_sentences[idx]}\")\n",
    "\n",
    "\n",
    "# Create an instance of the PatternExtractorWithGNN\n",
    "extractor = PatternExtractorWithGNN()\n",
    "\n",
    "# Step 1: Build the graph from sentences\n",
    "sentences_dict = {\n",
    "    'erinnere': [\n",
    "        'Ich erinnere mich gut',\n",
    "        'ich erinnere mich nicht',\n",
    "        'nochmal erinnere ich mich nicht',\n",
    "        'ich erinnere mich an das Treffen gestern',\n",
    "        'erinnere mich bitte daran, morgen früh aufzustehen',\n",
    "        'ich erinnere mich an die schöne Zeit'\n",
    "    ],\n",
    "    'erinnert': [\n",
    "        'wie erinnert man sich nochmal',\n",
    "        'wo erinnert man sich nochmal',\n",
    "        'vielleicht erinnert man sich dann nochmal',\n",
    "        'erinnert mich an meine Kindheit',\n",
    "        'sie erinnert sich nicht mehr an das Gespräch',\n",
    "        'er erinnert sich nicht gerne an die Vergangenheit'\n",
    "    ],\n",
    "    'erinnerte': [\n",
    "        'er erinnerte sich plötzlich an den Vorfall',\n",
    "        'ich erinnerte mich an mein erstes Auto',\n",
    "        'sie erinnerte sich, dass sie etwas vergessen hatte',\n",
    "        'erinnerte ich mich an den alten Freund',\n",
    "        'er erinnerte sich an die Worte seiner Mutter',\n",
    "        'ich erinnerte mich an den letzten Urlaub'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# language = 'de'\n",
    "# from melon_clustering import load_sentences, SENTENCES_DIR\n",
    "# sentences_dict = load_sentences(SENTENCES_DIR / 'erinnern.yaml', language)\n",
    "\n",
    "extractor.initialize(sentences_dict, overlap = 0)\n",
    "extractor.print_trees()\n",
    "# # Build graph from the sentences\n",
    "# edge_index = extractor.build_graph(sentences_dict)\n",
    "\n",
    "# # Step 2: Initialize node features\n",
    "# node_features = extractor.initialize_node_features(feature_dim=100)\n",
    "\n",
    "# # Step 3: Train GNN and get embeddings\n",
    "# gnn_node_embeddings = extractor.train_gnn(edge_index, node_features)\n",
    "\n",
    "# # Rest of your code\n",
    "# for steepness in [0.5, 1, 2, 3]:\n",
    "#     sentence_embeddings = []\n",
    "#     sentence_list = []\n",
    "#     for morphology, sentences in sentences_dict.items():\n",
    "#         for sentence in sentences:\n",
    "#             embedding = extractor.generate_sentence_embeddings(gnn_node_embeddings, sentence, morphology, steepness=steepness)\n",
    "#             sentence_embeddings.append(embedding)\n",
    "#             sentence_list.append(sentence)\n",
    "\n",
    "#     # Convert the list of embeddings to a NumPy array\n",
    "#     sentence_embeddings = np.array(sentence_embeddings)\n",
    "\n",
    "#     # Step 5: Cluster the embeddings\n",
    "#     clusters = extractor.cluster_embeddings(sentence_embeddings, n_clusters=3)\n",
    "\n",
    "#     # Step 6: Apply dimensionality reduction methods (PCA, t-SNE)\n",
    "#     for method in ['pca', 'tsne']:\n",
    "#         reduced_embeddings = extractor.reduce_dimensionality(sentence_embeddings, method=method)\n",
    "\n",
    "#         # Step 7: Visualize clusters using different methods\n",
    "#         extractor.visualize_clusters(reduced_embeddings, clusters, method=method, appendix=steepness)\n",
    "\n",
    "#     # Step 8: Print 3 central and 3 edge sentences for each cluster\n",
    "#     print_central_and_edge_sentences(sentence_embeddings, clusters, sentence_list, n=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
