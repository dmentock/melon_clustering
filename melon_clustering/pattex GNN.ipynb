{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from melon_clustering import PatternExtractor\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # First GCN layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        # Second GCN layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, word, id):\n",
    "        self.word = word\n",
    "        self.id = id\n",
    "        self.count = 0\n",
    "        self.children = {}\n",
    "\n",
    "class PatternExtractorWithGNN(PatternExtractor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.graph = nx.DiGraph()  # Graph to store word relations\n",
    "        self.node_embeddings = {}  # To store GCN embeddings\n",
    "\n",
    "    def set_up_digraph(self, node_id, parsed_nodes):\n",
    "        if node_id not in parsed_nodes:\n",
    "            parsed_nodes.add(node_id)\n",
    "            for child_node in self.id_to_node.get(node_id).children.values():\n",
    "                self.graph.add_edge(node_id, child_node.id)\n",
    "                self.set_up_digraph(child_node.id, parsed_nodes)\n",
    "\n",
    "    def build_graph(self):\n",
    "        # Convert the tree into a graph structure for GNN training\n",
    "        edge_index = torch.tensor(list(self.graph.edges)).t().contiguous()\n",
    "        return edge_index\n",
    "\n",
    "    def initialize_node_features(self, feature_dim=100):\n",
    "        # Initialize node features randomly\n",
    "        num_nodes = self.node_counter\n",
    "        node_features = torch.randn((num_nodes, feature_dim), requires_grad=True)\n",
    "        return node_features\n",
    "\n",
    "    def train_gnn(self, edge_index, node_features, hidden_dim=64, output_dim=100, epochs=200):\n",
    "        # Train the GNN model\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = GCN(input_dim=node_features.shape[1], hidden_dim=hidden_dim, output_dim=output_dim).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        # Move data to device\n",
    "        edge_index = edge_index.to(device)\n",
    "        node_features = node_features.to(device)\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            out = model(node_features, edge_index)\n",
    "            loss = F.mse_loss(out, node_features)  # Unsupervised loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "        return out.detach().cpu().numpy()\n",
    "\n",
    "    def generate_sentence_embeddings(self, gnn_node_embeddings, sentence, morphology, steepness=1.0):\n",
    "        # Generate sentence embeddings based on the GNN node embeddings\n",
    "        words = sentence.lower().split()\n",
    "        node_path = deque([0])\n",
    "        key_word_index = words.index(morphology.lower())\n",
    "        words_before = words[:key_word_index]\n",
    "        words_after = words[key_word_index + 1:]\n",
    "\n",
    "        current_node = self.preceding_tree\n",
    "        for word in words_before[::-1]:\n",
    "            if word in current_node.children:\n",
    "                current_node = current_node.children[word]\n",
    "                node_path.appendleft(current_node.id)\n",
    "\n",
    "        current_node = self.following_tree\n",
    "        for word in words_after:\n",
    "            if word in current_node.children:\n",
    "                current_node = current_node.children[word]\n",
    "                node_path.append(current_node.id)\n",
    "\n",
    "        return self.compute_weighted_sentence_embedding(gnn_node_embeddings, node_path, steepness=steepness)\n",
    "\n",
    "    def compute_weighted_sentence_embedding(self, gnn_node_embeddings, sentence_path, steepness=1.0):\n",
    "        # Compute the weighted sentence embedding based on the node embeddings\n",
    "        weighted_embedding = np.zeros(gnn_node_embeddings.shape[1])\n",
    "        for i, node_id in enumerate(sentence_path):\n",
    "            weight = 1 - (1 / (1 + np.exp(-steepness * (i + 1))))\n",
    "            weighted_embedding += weight * gnn_node_embeddings[node_id]\n",
    "        return weighted_embedding / len(sentence_path)\n",
    "\n",
    "extractor = PatternExtractorWithGNN()\n",
    "sentences_dict = {\n",
    "    'erinnere': [\n",
    "        'Ich erinnere mich gut',\n",
    "        'ich erinnere mich nicht',\n",
    "        'nochmal erinnere ich mich nicht',\n",
    "        'ich erinnere mich an das Treffen gestern',\n",
    "        'erinnere mich bitte daran, morgen früh aufzustehen',\n",
    "        'ich erinnere mich an die schöne Zeit'\n",
    "    ],\n",
    "    'erinnert': [\n",
    "        'wie erinnert man sich nochmal',\n",
    "        'wo erinnert man sich nochmal',\n",
    "        'vielleicht erinnert man sich dann nochmal',\n",
    "        'erinnert mich an meine Kindheit',\n",
    "        'sie erinnert sich nicht mehr an das Gespräch',\n",
    "        'er erinnert sich nicht gerne an die Vergangenheit'\n",
    "    ],\n",
    "    'erinnerte': [\n",
    "        'er erinnerte sich plötzlich an den Vorfall',\n",
    "        'ich erinnerte mich an mein erstes Auto',\n",
    "        'sie erinnerte sich, dass sie etwas vergessen hatte',\n",
    "        'erinnerte ich mich an den alten Freund',\n",
    "        'er erinnerte sich an die Worte seiner Mutter',\n",
    "        'ich erinnerte mich an den letzten Urlaub'\n",
    "    ]\n",
    "}\n",
    "extractor.initialize(sentences_dict, overlap_threshold = 0.4)\n",
    "extractor.print_trees()\n",
    "\n",
    "extractor.set_up_digraph(0, set())\n",
    "extractor.set_up_digraph(1, set())\n",
    "# Step 3: Build graph from the optimized tree\n",
    "edge_index = extractor.build_graph()\n",
    "\n",
    "# Step 4: Initialize node features\n",
    "node_features = extractor.initialize_node_features(feature_dim=100)\n",
    "\n",
    "# Step 5: Train GNN and get embeddings\n",
    "gnn_node_embeddings = extractor.train_gnn(edge_index, node_features)\n",
    "\n",
    "# Step 6: Perform clustering and visualize\n",
    "\n",
    "for sigmoid_steepness in [0.5, 1, 2]:\n",
    "    sentence_embeddings = []\n",
    "    sentence_list = []\n",
    "    for morphology, sentences in sentences_dict.items():\n",
    "        for sentence in sentences:\n",
    "            embedding = extractor.generate_sentence_embeddings(gnn_node_embeddings, sentence, morphology, steepness=sigmoid_steepness)\n",
    "            sentence_embeddings.append(embedding)\n",
    "            sentence_list.append(sentence)\n",
    "\n",
    "    # Convert the list of embeddings to a NumPy array\n",
    "    sentence_embeddings = np.array(sentence_embeddings)\n",
    "\n",
    "    # Step 7: Cluster the embeddings\n",
    "    clusters = extractor.cluster_embeddings(sentence_embeddings, n_clusters=3)\n",
    "\n",
    "    # Step 8: Apply dimensionality reduction and visualize clusters\n",
    "    for method in ['pca', 'tsne']:\n",
    "        reduced_embeddings = extractor.reduce_dimensionality(sentence_embeddings, method=method)\n",
    "        extractor.visualize_clusters(reduced_embeddings, clusters, sentence_list, method=method)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
